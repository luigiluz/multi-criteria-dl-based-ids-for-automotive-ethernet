{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Clone git repository"
      ],
      "metadata": {
        "id": "HZ-osT5NY1f_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gzivjipYwcq",
        "outputId": "07188540-6a3e-415a-daa3-4affeb902cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LilNetX'...\n",
            "remote: Enumerating objects: 311, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 311 (delta 44), reused 37 (delta 17), pack-reused 245\u001b[K\n",
            "Receiving objects: 100% (311/311), 87.18 KiB | 2.03 MiB/s, done.\n",
            "Resolving deltas: 100% (202/202), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/luigiluz/LilNetX.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount google drive"
      ],
      "metadata": {
        "id": "Bz0fSA7ZTqtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9LMbRQ2Tp0B",
        "outputId": "dc443521-681f-467e-bc54-d3296fb7d8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install venv to create and isolated environment"
      ],
      "metadata": {
        "id": "C3RsyXrPY2-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install python3.8-venv ninja-build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08tIxUcGY5dH",
        "outputId": "0511e962-6418-4cf9-aa2c-ba1c9e4e6a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  ninja-build python3.8-venv\n",
            "0 upgraded, 2 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 2,794 kB of archives.\n",
            "After this operation, 3,231 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.8-venv amd64 3.8.16-1+bionic1 [2,701 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 ninja-build amd64 1.8.2-1 [93.3 kB]\n",
            "Fetched 2,794 kB in 1s (2,689 kB/s)\n",
            "Selecting previously unselected package ninja-build.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../ninja-build_1.8.2-1_amd64.deb ...\n",
            "Unpacking ninja-build (1.8.2-1) ...\n",
            "Selecting previously unselected package python3.8-venv.\n",
            "Preparing to unpack .../python3.8-venv_3.8.16-1+bionic1_amd64.deb ...\n",
            "Unpacking python3.8-venv (3.8.16-1+bionic1) ...\n",
            "Setting up python3.8-venv (3.8.16-1+bionic1) ...\n",
            "Setting up ninja-build (1.8.2-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create virtual environment and install dependencies"
      ],
      "metadata": {
        "id": "EzaDHrv9ZZNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m venv venv"
      ],
      "metadata": {
        "id": "aWnN3XE9Z0VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!source venv/bin/activate"
      ],
      "metadata": {
        "id": "m85WVvJiZct3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r LilNetX/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJNrGqxRZe_v",
        "outputId": "3e5a53e4-d353-4a0e-8917-cc5b4f3b6c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.10.2\n",
            "  Downloading torch-1.10.2-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
            "\u001b[K     |██████████████████████████████▎ | 834.1 MB 1.1 MB/s eta 0:00:43tcmalloc: large alloc 1147494400 bytes == 0x65c80000 @  0x7f1fa5d9c615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116 0x5d8d8c 0x55dc1e 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4990ca 0x5d8868 0x4997a2 0x4fd8b5 0x49abe4\n",
            "\u001b[K     |████████████████████████████████| 881.9 MB 1.4 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.11.3\n",
            "  Downloading torchvision-0.11.3-cp38-cp38-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from -r LilNetX/requirements.txt (line 3)) (5.4.8)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.8/dist-packages (from -r LilNetX/requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from -r LilNetX/requirements.txt (line 5)) (6.0)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Collecting pytorch_pfn_extras\n",
            "  Downloading pytorch-pfn-extras-0.6.3.tar.gz (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 82.6 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fastargs\n",
            "  Downloading fastargs-1.2.0.tar.gz (12 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from -r LilNetX/requirements.txt (line 9)) (3.2.2)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Collecting imgcat\n",
            "  Downloading imgcat-0.5.0.tar.gz (9.9 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from -r LilNetX/requirements.txt (line 12)) (1.3.5)\n",
            "Collecting assertpy\n",
            "  Downloading assertpy-1.1.tar.gz (25 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from -r LilNetX/requirements.txt (line 14)) (4.64.1)\n",
            "Collecting webdataset\n",
            "  Downloading webdataset-0.2.31-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 76.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from -r LilNetX/requirements.txt (line 17)) (4.6.0.66)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from -r LilNetX/requirements.txt (line 18)) (0.56.4)\n",
            "Collecting wandb==0.13.6\n",
            "  Downloading wandb-0.13.6-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 61.9 MB/s \n",
            "\u001b[?25hCollecting torchac==0.9.3\n",
            "  Downloading torchac-0.9.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.2->-r LilNetX/requirements.txt (line 1)) (4.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.3->-r LilNetX/requirements.txt (line 2)) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.11.3->-r LilNetX/requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (57.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (2.3)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (3.19.6)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 69.6 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (7.1.2)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.12.0-py2.py3-none-any.whl (173 kB)\n",
            "\u001b[K     |████████████████████████████████| 173 kB 46.7 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (2.23.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb==0.13.6->-r LilNetX/requirements.txt (line 19)) (2.10)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.11.1-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 65.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.11.0-py2.py3-none-any.whl (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 61.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 78.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.10.0-py2.py3-none-any.whl (166 kB)\n",
            "\u001b[K     |████████████████████████████████| 166 kB 77.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.10-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 77.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.9-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[K     |████████████████████████████████| 162 kB 81.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 75.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 85.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 81.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 80.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 71.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 78.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 62.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from pytorch_pfn_extras->-r LilNetX/requirements.txt (line 7)) (21.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r LilNetX/requirements.txt (line 9)) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r LilNetX/requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->-r LilNetX/requirements.txt (line 9)) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->-r LilNetX/requirements.txt (line 12)) (2022.6)\n",
            "Collecting braceexpand\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->-r LilNetX/requirements.txt (line 18)) (5.1.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->-r LilNetX/requirements.txt (line 18)) (0.39.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->-r LilNetX/requirements.txt (line 18)) (3.11.0)\n",
            "Building wheels for collected packages: pytorch-pfn-extras, fastargs, sklearn, imgcat, assertpy, pathtools\n",
            "  Building wheel for pytorch-pfn-extras (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-pfn-extras: filename=pytorch_pfn_extras-0.6.3-py3-none-any.whl size=182861 sha256=16f2f8bbf04fa3c8bb465dc31867014c6f69fe7ae0ba516b4de7d8501f9d90da\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/b1/e3/cd41c02e5f9b5506c1faa2652f47421d8aae1253bf17aa370b\n",
            "  Building wheel for fastargs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastargs: filename=fastargs-1.2.0-py3-none-any.whl size=10926 sha256=4f90f3c5cd1d061c1aba0a350a31aa191d35a26ecd29c077afb503bb73ad1e71\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/05/09/44e0430a8e6413590ad62fb73e44c283b835f8ee3e2eb8c473\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=0a462d89064c2c70d900f822a2abea685e30b17c5158505c36663c0763639844\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "  Building wheel for imgcat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgcat: filename=imgcat-0.5.0-py3-none-any.whl size=10507 sha256=9f3c0154a65f34642e5e6b9507d6801e8c8bd390665841588dc4e668c6c4ef0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/44/71/4d/36f3fa465ff90f43b1e27aabdfb7fc08833efcce90de49ed4c\n",
            "  Building wheel for assertpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for assertpy: filename=assertpy-1.1-py3-none-any.whl size=42917 sha256=ff9b340c0f8690618b8b8d43f632948ec0c453a7377727a64bd6af8e772c3a19\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/86/c9/1310be6ddfb540daa0bf1ac204526837aa0a8b0e79f32855ff\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=cfe1cc0fa9044f091605c15680acb05c1c84fd0e08353c2a71c6a2694f7214a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pytorch-pfn-extras fastargs sklearn imgcat assertpy pathtools\n",
            "Installing collected packages: smmap, gitdb, torch, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, braceexpand, webdataset, wandb, torchvision, torchmetrics, torchac, terminaltables, sklearn, pytorch-pfn-extras, imgcat, fastargs, assertpy\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.0+cu116\n",
            "    Uninstalling torchvision-0.14.0+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.10.2 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.10.2 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.29 assertpy-1.1 braceexpand-0.1.7 docker-pycreds-0.4.0 fastargs-1.2.0 gitdb-4.0.10 imgcat-0.5.0 pathtools-0.1.2 pytorch-pfn-extras-0.6.3 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.11 sklearn-0.0.post1 smmap-5.0.0 terminaltables-3.1.10 torch-1.10.2 torchac-0.9.3 torchmetrics-0.11.0 torchvision-0.11.3 wandb-0.13.6 webdataset-0.2.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update repository reference"
      ],
      "metadata": {
        "id": "7gbitZ7cogXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LilNetX/\n",
        "!git fetch origin\n",
        "!git checkout -B run origin/main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQH1O4lmogLH",
        "outputId": "86b66271-b0b3-4c53-85b1-008c67edaf84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the corresponding code"
      ],
      "metadata": {
        "id": "6vwFKgQtbgwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('always')"
      ],
      "metadata": {
        "id": "Rv-hrqKdhIpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruned model"
      ],
      "metadata": {
        "id": "wY-9HShznpte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 LilNetX/main.py --config LilNetX/configs/avtp_intrusion_cnn_ids_inf.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhIzUzuGnpaC",
        "outputId": "8d5648d2-f524-4f9b-a582-7cb640bb3931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config loaded from:  /content/LilNetX/configs/avtp_intrusion_cnn_ids_inf.yaml\n",
            "Running evaluation only\n",
            "Changed random seed to 901121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id c0ee252573fd6829c5a76dcdd33c55df.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "tcmalloc: large alloc 3283222528 bytes == 0x77d0000 @  0x7f34f97831e7 0x7f34972a014e 0x7f34972f8745 0x7f34972a39c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4bddd1 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Trainset has 446372 bening Xis and 196892 injected Xis\n",
            "tcmalloc: large alloc 3283222528 bytes == 0xce0b6000 @  0x7f34f97831e7 0x7f34972a014e 0x7f34972f8745 0x7f34972a39c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4b8c8c 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Testset has 446372 bening Xis and 196892 injected Xis\n",
            "convnetids var = 0.00439882697947214\n",
            "classname = Conv2d, fan = 675, boundary = 1.0068977207298249\n",
            "classname = Conv2d, fan = 650, boundary = 1.032469098697112\n",
            "classname = Linear, fan = 64, boundary = 4.143543905251678\n",
            "classname = Linear, fan = 1, boundary = 36.43575503492517\n",
            "Number of parameters: 0.5507 M\n",
            "Checkpoint found, continuing training from epoch 10\n",
            "Changing random seed to 901121\n",
            "Mean inference time = 1589.5787104964256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_pruned_fold0/wandb/offline-run-20221219_053442-c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_pruned_fold0/wandb/offline-run-20221219_053442-c0ee252573fd6829c5a76dcdd33c55df/logs\u001b[0m\n",
            "Total time taken: 94.57s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation set"
      ],
      "metadata": {
        "id": "P6gcyeQTHMqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 LilNetX/main.py --config LilNetX/configs/val_avtp/avtp_intrusion_cnn_ids_val_2.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5EfGvy3Zw1w",
        "outputId": "99004602-8abe-47a3-e19f-6d583afba243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config loaded from:  /content/LilNetX/configs/val_avtp/avtp_intrusion_cnn_ids_val_2.yaml\n",
            "{'gpu_names': 'Tesla T4', 'gpu_count': 1, 'CUDA_VISIBLE_DEVICES': 'NotSet', 'cudnn.enabled': True, 'cudnn.benchmark': True, 'cudnn.deterministic': False, 'cudnn.version': 7605}\n",
            "{'memory': 'svmem(total=27331215360, available=25989599232, percent=4.9, used=1023746048, free=18521108480, active=1827901440, inactive=6413746176, buffers=414150656, cached=7372210176, shared=1216512, slab=404275200)'}\n",
            "{'CPUs': '4'}\n",
            "Running evaluation only\n",
            "Changed random seed to 805354\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlfml\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold2/wandb/run-20221216_045824-c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "tcmalloc: large alloc 3283222528 bytes == 0x6d94000 @  0x7f6a2f36a1e7 0x7f69cce8714e 0x7f69ccedf745 0x7f69cce8a9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4bddd1 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Trainset has 446372 bening Xis and 196892 injected Xis\n",
            "tcmalloc: large alloc 3283222528 bytes == 0xd4df6000 @  0x7f6a2f36a1e7 0x7f69cce8714e 0x7f69ccedf745 0x7f69cce8a9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4b8c8c 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Testset has 446372 bening Xis and 196892 injected Xis\n",
            "convnetids var = 0.003906249999999999\n",
            "classname = Conv2d, fan = 800, boundary = 0.9730919862656238\n",
            "classname = Conv2d, fan = 1600, boundary = 0.6000000000000001\n",
            "classname = Linear, fan = 64, boundary = 4.424428900898053\n",
            "classname = Linear, fan = 1, boundary = 38.69502519453203\n",
            "Number of parameters: 1.3605 M\n",
            "Using /root/.cache/torch_extensions/py38_cu102 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu102/torchac_backend/build.ninja...\n",
            "Building extension module torchac_backend...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module torchac_backend...\n",
            "Checkpoint found, continuing training from epoch 4\n",
            "Changing random seed to 805354\n",
            "Test: [  0/503]\tTime  0.576 ( 0.576)\tLoss 5.9954e-01 (5.9954e-01)\tAcc@1 0.95703125 (0.95703125)\n",
            "Test: [ 50/503]\tTime  0.024 ( 0.035)\tLoss 5.9149e-01 (5.9182e-01)\tAcc@1 0.9609375 (0.9626225490196079)\n",
            "Test: [100/503]\tTime  0.024 ( 0.030)\tLoss 5.7665e-01 (5.9239e-01)\tAcc@1 0.984375 (0.9626392326732673)\n",
            "Test: [150/503]\tTime  0.025 ( 0.028)\tLoss 6.0993e-01 (5.9199e-01)\tAcc@1 0.96875 (0.9629552980132451)\n",
            "Test: [200/503]\tTime  0.025 ( 0.028)\tLoss 5.9988e-01 (5.9232e-01)\tAcc@1 0.95703125 (0.9636777052238806)\n",
            "Test: [250/503]\tTime  0.025 ( 0.027)\tLoss 5.7578e-01 (5.9209e-01)\tAcc@1 0.95703125 (0.9635831673306773)\n",
            "Test: [300/503]\tTime  0.029 ( 0.027)\tLoss 5.8223e-01 (5.9252e-01)\tAcc@1 0.97265625 (0.9635459925249169)\n",
            "Test: [350/503]\tTime  0.024 ( 0.027)\tLoss 5.9640e-01 (5.9252e-01)\tAcc@1 0.94921875 (0.9638198896011396)\n",
            "Test: [400/503]\tTime  0.024 ( 0.027)\tLoss 5.8591e-01 (5.9290e-01)\tAcc@1 0.95703125 (0.9635481608478803)\n",
            "Test: [450/503]\tTime  0.024 ( 0.026)\tLoss 5.8076e-01 (5.9282e-01)\tAcc@1 0.96484375 (0.9634925859201774)\n",
            "Test: [500/503]\tTime  0.024 ( 0.026)\tLoss 5.8077e-01 (5.9286e-01)\tAcc@1 0.97265625 (0.9632063997005988)\n",
            "Test: [502/503]\tTime  0.080 ( 0.026)\tLoss 5.8996e-01 (5.9285e-01)\tAcc@1 0.9787234042553191 (0.9632499825110957)\n",
            "/content/LilNetX/trainer.py:1008: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  'Net Bytes {bytes}'.format(bytes=bits//8000) if not conf_network['vanilla'] else ''+ \\\n",
            " * Acc@1 0.9632499825110957Net Bytes 61.0\n",
            "/content/LilNetX/trainer.py:1011: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self.wandb.log({\"loss_val\": losses.avg, \"top1_val\": top1.avg, \"net_bytes_val\": bits//8000, \\\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ac_bytes ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        f1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: net_bytes_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      prec_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    recall_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   roc_auc_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      top1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ac_bytes 61\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_bytes 57688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best_val_top1 0.94478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_train 0.57959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                f1_val 0.9375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           final_bytes 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        final_val_top1 0.81298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_reg_train 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train 0.65996\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              loss_val 0.59285\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             net_bytes 46.98259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         net_bytes_val 61.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            prec_train 0.81628\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              prec_val 0.9698\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          recall_train 0.45184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            recall_val 0.90802\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         roc_auc_train 0.70392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           roc_auc_val 0.96208\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        sparse_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       sparse_discrete 0.92425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sparse_in_decoded 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_in_discrete 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_out_decoded 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   sparse_out_discrete 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  sparse_slice_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: sparse_slice_discrete 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               timings 183.69458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            top1_train 0.80099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              top1_val 0.96325\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold2/wandb/run-20221216_045824-c0ee252573fd6829c5a76dcdd33c55df/logs\u001b[0m\n",
            "Total time taken: 44.82s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "!python3 LilNetX/main.py --config LilNetX/configs/val_avtp/avtp_intrusion_cnn_ids_val_3.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anycZLLNIJ8y",
        "outputId": "744c3ec0-c30a-4fac-b8a2-e7c50fcacd39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config loaded from:  /content/LilNetX/configs/val_avtp/avtp_intrusion_cnn_ids_val_3.yaml\n",
            "{'gpu_names': 'Tesla T4', 'gpu_count': 1, 'CUDA_VISIBLE_DEVICES': 'NotSet', 'cudnn.enabled': True, 'cudnn.benchmark': True, 'cudnn.deterministic': False, 'cudnn.version': 7605}\n",
            "{'memory': 'svmem(total=27331215360, available=25983660032, percent=4.9, used=1029210112, free=18513076224, active=1828716544, inactive=6419951616, buffers=414400512, cached=7374528512, shared=1216512, slab=405770240)'}\n",
            "{'CPUs': '4'}\n",
            "Running evaluation only\n",
            "Changed random seed to 805354\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlfml\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold3/wandb/run-20221216_045913-c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "tcmalloc: large alloc 3283222528 bytes == 0x8634000 @  0x7f031e2991e7 0x7f02bbdb614e 0x7f02bbe0e745 0x7f02bbdb99c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4bddd1 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Trainset has 446372 bening Xis and 196892 injected Xis\n",
            "tcmalloc: large alloc 3283222528 bytes == 0xd0498000 @  0x7f031e2991e7 0x7f02bbdb614e 0x7f02bbe0e745 0x7f02bbdb99c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4b8c8c 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Testset has 446372 bening Xis and 196892 injected Xis\n",
            "convnetids var = 0.003906249999999999\n",
            "classname = Conv2d, fan = 800, boundary = 0.9730919862656238\n",
            "classname = Conv2d, fan = 1600, boundary = 0.6000000000000001\n",
            "classname = Linear, fan = 64, boundary = 4.424428900898053\n",
            "classname = Linear, fan = 1, boundary = 38.69502519453203\n",
            "Number of parameters: 1.3605 M\n",
            "Using /root/.cache/torch_extensions/py38_cu102 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu102/torchac_backend/build.ninja...\n",
            "Building extension module torchac_backend...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module torchac_backend...\n",
            "Checkpoint found, continuing training from epoch 10\n",
            "Changing random seed to 805354\n",
            "Test: [  0/503]\tTime  0.574 ( 0.574)\tLoss 5.5960e-01 (5.5960e-01)\tAcc@1 0.99609375 (0.99609375)\n",
            "Test: [ 50/503]\tTime  0.024 ( 0.035)\tLoss 5.8413e-01 (5.8443e-01)\tAcc@1 0.98828125 (0.9865196078431373)\n",
            "Test: [100/503]\tTime  0.024 ( 0.030)\tLoss 5.9330e-01 (5.8385e-01)\tAcc@1 0.984375 (0.9865408415841584)\n",
            "Test: [150/503]\tTime  0.024 ( 0.028)\tLoss 5.8150e-01 (5.8383e-01)\tAcc@1 0.96875 (0.9864445364238411)\n",
            "Test: [200/503]\tTime  0.025 ( 0.027)\tLoss 5.9636e-01 (5.8418e-01)\tAcc@1 0.98828125 (0.9862989738805971)\n",
            "Test: [250/503]\tTime  0.026 ( 0.026)\tLoss 5.5998e-01 (5.8409e-01)\tAcc@1 0.98828125 (0.9866315986055777)\n",
            "Test: [300/503]\tTime  0.023 ( 0.026)\tLoss 5.6571e-01 (5.8372e-01)\tAcc@1 0.984375 (0.9865033222591362)\n",
            "Test: [350/503]\tTime  0.024 ( 0.026)\tLoss 5.8131e-01 (5.8353e-01)\tAcc@1 0.9921875 (0.9865006232193733)\n",
            "Test: [400/503]\tTime  0.024 ( 0.025)\tLoss 5.8757e-01 (5.8316e-01)\tAcc@1 0.98046875 (0.9867713528678305)\n",
            "Test: [450/503]\tTime  0.025 ( 0.025)\tLoss 5.8225e-01 (5.8320e-01)\tAcc@1 0.98828125 (0.9868001662971175)\n",
            "Test: [500/503]\tTime  0.023 ( 0.025)\tLoss 5.7092e-01 (5.8317e-01)\tAcc@1 0.984375 (0.9866750873253493)\n",
            "Test: [502/503]\tTime  0.077 ( 0.025)\tLoss 5.7176e-01 (5.8315e-01)\tAcc@1 0.9785714285714285 (0.9866927836333675)\n",
            "/content/LilNetX/trainer.py:1008: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  'Net Bytes {bytes}'.format(bytes=bits//8000) if not conf_network['vanilla'] else ''+ \\\n",
            " * Acc@1 0.9866927836333675Net Bytes 12.0\n",
            "/content/LilNetX/trainer.py:1011: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self.wandb.log({\"loss_val\": losses.avg, \"top1_val\": top1.avg, \"net_bytes_val\": bits//8000, \\\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ac_bytes ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        f1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: net_bytes_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      prec_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    recall_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   roc_auc_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      top1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ac_bytes 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_bytes 57688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best_val_top1 0.94478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_train 0.57959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                f1_val 0.97803\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           final_bytes 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        final_val_top1 0.81298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_reg_train 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train 0.65996\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              loss_val 0.58315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             net_bytes 46.98259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         net_bytes_val 12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            prec_train 0.81628\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              prec_val 0.98312\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          recall_train 0.45184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            recall_val 0.97328\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         roc_auc_train 0.70392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           roc_auc_val 0.99024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        sparse_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       sparse_discrete 0.92425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sparse_in_decoded 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_in_discrete 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_out_decoded 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   sparse_out_discrete 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  sparse_slice_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: sparse_slice_discrete 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               timings 183.69458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            top1_train 0.80099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              top1_val 0.98669\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold3/wandb/run-20221216_045913-c0ee252573fd6829c5a76dcdd33c55df/logs\u001b[0m\n",
            "Total time taken: 44.76s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "!python3 LilNetX/main.py --config LilNetX/configs/val_avtp/avtp_intrusion_cnn_ids_val_4.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTthdvMzIRQ6",
        "outputId": "1657d4b0-f147-4503-f8e7-d1ff03cd93a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config loaded from:  /content/LilNetX/configs/val_avtp/avtp_intrusion_cnn_ids_val_4.yaml\n",
            "{'gpu_names': 'Tesla T4', 'gpu_count': 1, 'CUDA_VISIBLE_DEVICES': 'NotSet', 'cudnn.enabled': True, 'cudnn.benchmark': True, 'cudnn.deterministic': False, 'cudnn.version': 7605}\n",
            "{'memory': 'svmem(total=27331215360, available=25983471616, percent=4.9, used=1029472256, free=18510422016, active=1829437440, inactive=6421827584, buffers=414699520, cached=7376621568, shared=1216512, slab=405692416)'}\n",
            "{'CPUs': '4'}\n",
            "Running evaluation only\n",
            "Changed random seed to 805354\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlfml\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold4/wandb/run-20221216_050001-c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "tcmalloc: large alloc 3283222528 bytes == 0x8538000 @  0x7f2f0aa8b1e7 0x7f2ea85a814e 0x7f2ea8600745 0x7f2ea85ab9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4bddd1 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Trainset has 446372 bening Xis and 196892 injected Xis\n",
            "tcmalloc: large alloc 3283222528 bytes == 0xd039c000 @  0x7f2f0aa8b1e7 0x7f2ea85a814e 0x7f2ea8600745 0x7f2ea85ab9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4b8c8c 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Testset has 446372 bening Xis and 196892 injected Xis\n",
            "convnetids var = 0.003906249999999999\n",
            "classname = Conv2d, fan = 800, boundary = 0.9730919862656238\n",
            "classname = Conv2d, fan = 1600, boundary = 0.6000000000000001\n",
            "classname = Linear, fan = 64, boundary = 4.424428900898053\n",
            "classname = Linear, fan = 1, boundary = 38.69502519453203\n",
            "Number of parameters: 1.3605 M\n",
            "Using /root/.cache/torch_extensions/py38_cu102 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu102/torchac_backend/build.ninja...\n",
            "Building extension module torchac_backend...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module torchac_backend...\n",
            "Checkpoint found, continuing training from epoch 10\n",
            "Changing random seed to 805354\n",
            "Test: [  0/503]\tTime  0.559 ( 0.559)\tLoss 5.6601e-01 (5.6601e-01)\tAcc@1 0.98046875 (0.98046875)\n",
            "Test: [ 50/503]\tTime  0.024 ( 0.034)\tLoss 5.8041e-01 (5.8607e-01)\tAcc@1 0.99609375 (0.9822303921568627)\n",
            "Test: [100/503]\tTime  0.024 ( 0.029)\tLoss 5.9506e-01 (5.8554e-01)\tAcc@1 0.98046875 (0.9823251856435643)\n",
            "Test: [150/503]\tTime  0.024 ( 0.027)\tLoss 5.7873e-01 (5.8541e-01)\tAcc@1 0.9765625 (0.9824348096026491)\n",
            "Test: [200/503]\tTime  0.023 ( 0.026)\tLoss 5.9911e-01 (5.8581e-01)\tAcc@1 0.98046875 (0.9820817786069652)\n",
            "Test: [250/503]\tTime  0.023 ( 0.026)\tLoss 5.6187e-01 (5.8581e-01)\tAcc@1 0.984375 (0.982351842629482)\n",
            "Test: [300/503]\tTime  0.023 ( 0.025)\tLoss 5.6923e-01 (5.8534e-01)\tAcc@1 0.9765625 (0.9824153862126246)\n",
            "Test: [350/503]\tTime  0.024 ( 0.025)\tLoss 5.8279e-01 (5.8511e-01)\tAcc@1 0.98828125 (0.9825498575498576)\n",
            "Test: [400/503]\tTime  0.023 ( 0.025)\tLoss 5.8674e-01 (5.8482e-01)\tAcc@1 0.98046875 (0.9826702774314214)\n",
            "Test: [450/503]\tTime  0.024 ( 0.025)\tLoss 5.8281e-01 (5.8482e-01)\tAcc@1 0.984375 (0.9827466740576497)\n",
            "Test: [500/503]\tTime  0.022 ( 0.025)\tLoss 5.7061e-01 (5.8479e-01)\tAcc@1 0.984375 (0.9826362899201597)\n",
            "Test: [502/503]\tTime  0.077 ( 0.025)\tLoss 5.7318e-01 (5.8478e-01)\tAcc@1 0.9714285714285714 (0.9826353263066256)\n",
            "/content/LilNetX/trainer.py:1008: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  'Net Bytes {bytes}'.format(bytes=bits//8000) if not conf_network['vanilla'] else ''+ \\\n",
            " * Acc@1 0.9826353263066256Net Bytes 12.0\n",
            "/content/LilNetX/trainer.py:1011: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self.wandb.log({\"loss_val\": losses.avg, \"top1_val\": top1.avg, \"net_bytes_val\": bits//8000, \\\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ac_bytes ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        f1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: net_bytes_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      prec_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    recall_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   roc_auc_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      top1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ac_bytes 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_bytes 57688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best_val_top1 0.94478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_train 0.57959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                f1_val 0.97121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           final_bytes 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        final_val_top1 0.81298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_reg_train 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train 0.65996\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              loss_val 0.58478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             net_bytes 46.98259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         net_bytes_val 12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            prec_train 0.81628\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              prec_val 0.98113\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          recall_train 0.45184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            recall_val 0.96182\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         roc_auc_train 0.70392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           roc_auc_val 0.98222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        sparse_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       sparse_discrete 0.92425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sparse_in_decoded 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_in_discrete 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_out_decoded 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   sparse_out_discrete 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  sparse_slice_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: sparse_slice_discrete 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               timings 183.69458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            top1_train 0.80099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              top1_val 0.98264\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold4/wandb/run-20221216_050001-c0ee252573fd6829c5a76dcdd33c55df/logs\u001b[0m\n",
            "Total time taken: 44.97s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Test set"
      ],
      "metadata": {
        "id": "7AO8FJ6KHKNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "!python3 LilNetX/main.py --config LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_1.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNJ6rrYeElVV",
        "outputId": "cea5635b-18ea-43a0-a230-1ab0fbf634e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config loaded from:  /content/LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_1.yaml\n",
            "{'gpu_names': 'Tesla T4', 'gpu_count': 1, 'CUDA_VISIBLE_DEVICES': 'NotSet', 'cudnn.enabled': True, 'cudnn.benchmark': True, 'cudnn.deterministic': False, 'cudnn.version': 7605}\n",
            "{'memory': 'svmem(total=27331215360, available=25796628480, percent=5.6, used=1243762688, free=18189533184, active=1772212224, inactive=6779568128, buffers=407756800, cached=7490162688, shared=1216512, slab=410050560)'}\n",
            "{'CPUs': '4'}\n",
            "Running evaluation only\n",
            "Changed random seed to 805354\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlfml\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold1/wandb/run-20221216_041330-c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "tcmalloc: large alloc 3283222528 bytes == 0x7390000 @  0x7f087982c1e7 0x7f081734914e 0x7f08173a1745 0x7f081734c9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4bddd1 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Trainset has 446372 bening Xis and 196892 injected Xis\n",
            "tcmalloc: large alloc 9546981376 bytes == 0x7f0590b0a000 @  0x7f087982c1e7 0x7f081734914e 0x7f08173a1745 0x7f081734c9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4b8c8c 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Testset has 1494253 bening Xis and 376236 injected Xis\n",
            "convnetids var = 0.003906249999999999\n",
            "classname = Conv2d, fan = 800, boundary = 0.9730919862656238\n",
            "classname = Conv2d, fan = 1600, boundary = 0.6000000000000001\n",
            "classname = Linear, fan = 64, boundary = 4.424428900898053\n",
            "classname = Linear, fan = 1, boundary = 38.69502519453203\n",
            "Number of parameters: 1.3605 M\n",
            "Using /root/.cache/torch_extensions/py38_cu102 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu102/torchac_backend/build.ninja...\n",
            "Building extension module torchac_backend...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module torchac_backend...\n",
            "Checkpoint found, continuing training from epoch 10\n",
            "Changing random seed to 805354\n",
            "Test: [   0/7307]\tTime  0.590 ( 0.590)\tLoss 6.1778e-01 (6.1778e-01)\tAcc@1 0.9921875 (0.9921875)\n",
            "Test: [  50/7307]\tTime  0.023 ( 0.035)\tLoss 6.2781e-01 (6.2029e-01)\tAcc@1 0.99609375 (0.9938725490196079)\n",
            "Test: [ 100/7307]\tTime  0.026 ( 0.030)\tLoss 6.1747e-01 (6.1993e-01)\tAcc@1 1.0 (0.9940826113861386)\n",
            "Test: [ 150/7307]\tTime  0.023 ( 0.028)\tLoss 6.1516e-01 (6.2004e-01)\tAcc@1 0.99609375 (0.9937655215231788)\n",
            "Test: [ 200/7307]\tTime  0.023 ( 0.027)\tLoss 6.0502e-01 (6.2002e-01)\tAcc@1 0.99609375 (0.9935673196517413)\n",
            "Test: [ 250/7307]\tTime  0.023 ( 0.026)\tLoss 6.1638e-01 (6.1927e-01)\tAcc@1 0.98828125 (0.9935414591633466)\n",
            "Test: [ 300/7307]\tTime  0.023 ( 0.026)\tLoss 6.0504e-01 (6.1942e-01)\tAcc@1 0.9921875 (0.9935631229235881)\n",
            "Test: [ 350/7307]\tTime  0.024 ( 0.025)\tLoss 6.1895e-01 (6.1963e-01)\tAcc@1 1.0 (0.9935340990028491)\n",
            "Test: [ 400/7307]\tTime  0.023 ( 0.025)\tLoss 6.1440e-01 (6.1978e-01)\tAcc@1 0.9921875 (0.9934343827930174)\n",
            "Test: [ 450/7307]\tTime  0.024 ( 0.025)\tLoss 6.2077e-01 (6.2004e-01)\tAcc@1 0.98828125 (0.9933481152993349)\n",
            "Test: [ 500/7307]\tTime  0.023 ( 0.025)\tLoss 6.3260e-01 (6.2013e-01)\tAcc@1 0.98828125 (0.9932322854291418)\n",
            "Test: [ 550/7307]\tTime  0.026 ( 0.025)\tLoss 6.0708e-01 (6.2002e-01)\tAcc@1 0.99609375 (0.9933714269509982)\n",
            "Test: [ 600/7307]\tTime  0.024 ( 0.025)\tLoss 6.1933e-01 (6.2012e-01)\tAcc@1 0.98828125 (0.9933639247088186)\n",
            "Test: [ 650/7307]\tTime  0.025 ( 0.025)\tLoss 6.2076e-01 (6.2016e-01)\tAcc@1 0.99609375 (0.9934475806451613)\n",
            "Test: [ 700/7307]\tTime  0.025 ( 0.025)\tLoss 6.2332e-01 (6.2023e-01)\tAcc@1 0.98828125 (0.9934189996433667)\n",
            "Test: [ 750/7307]\tTime  0.024 ( 0.025)\tLoss 6.1638e-01 (6.2006e-01)\tAcc@1 0.9921875 (0.9934358355525965)\n",
            "Test: [ 800/7307]\tTime  0.024 ( 0.025)\tLoss 6.1396e-01 (6.2004e-01)\tAcc@1 0.99609375 (0.9934359394506866)\n",
            "Test: [ 850/7307]\tTime  0.024 ( 0.025)\tLoss 6.1712e-01 (6.1991e-01)\tAcc@1 0.99609375 (0.9934498017038778)\n",
            "Test: [ 900/7307]\tTime  0.023 ( 0.025)\tLoss 6.3478e-01 (6.1992e-01)\tAcc@1 0.9921875 (0.9934187708102109)\n",
            "Test: [ 950/7307]\tTime  0.024 ( 0.025)\tLoss 6.0615e-01 (6.1993e-01)\tAcc@1 0.99609375 (0.9934772607781283)\n",
            "Test: [1000/7307]\tTime  0.024 ( 0.024)\tLoss 6.2789e-01 (6.1999e-01)\tAcc@1 1.0 (0.9934869817682318)\n",
            "Test: [1050/7307]\tTime  0.024 ( 0.024)\tLoss 6.1895e-01 (6.2000e-01)\tAcc@1 0.99609375 (0.993506927925785)\n",
            "Test: [1100/7307]\tTime  0.024 ( 0.024)\tLoss 6.2780e-01 (6.1994e-01)\tAcc@1 0.984375 (0.9935073228882834)\n",
            "Test: [1150/7307]\tTime  0.024 ( 0.024)\tLoss 6.2137e-01 (6.1987e-01)\tAcc@1 0.9921875 (0.9935246524761078)\n",
            "Test: [1200/7307]\tTime  0.025 ( 0.024)\tLoss 6.1467e-01 (6.1988e-01)\tAcc@1 0.98828125 (0.9934884991673605)\n",
            "Test: [1250/7307]\tTime  0.024 ( 0.024)\tLoss 6.2583e-01 (6.1988e-01)\tAcc@1 0.99609375 (0.9934489908073542)\n",
            "Test: [1300/7307]\tTime  0.024 ( 0.024)\tLoss 5.9374e-01 (6.2003e-01)\tAcc@1 0.99609375 (0.9934485491929285)\n",
            "Test: [1350/7307]\tTime  0.024 ( 0.024)\tLoss 6.3082e-01 (6.2015e-01)\tAcc@1 1.0 (0.9934365747594375)\n",
            "Test: [1400/7307]\tTime  0.025 ( 0.024)\tLoss 6.2638e-01 (6.2015e-01)\tAcc@1 1.0 (0.9934672778372591)\n",
            "Test: [1450/7307]\tTime  0.024 ( 0.024)\tLoss 6.2518e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9934420227429359)\n",
            "Test: [1500/7307]\tTime  0.024 ( 0.024)\tLoss 6.2978e-01 (6.2011e-01)\tAcc@1 1.0 (0.9934704988341105)\n",
            "Test: [1550/7307]\tTime  0.024 ( 0.024)\tLoss 6.2434e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9934618794326241)\n",
            "Test: [1600/7307]\tTime  0.023 ( 0.024)\tLoss 6.3012e-01 (6.2018e-01)\tAcc@1 0.9921875 (0.9934586781698939)\n",
            "Test: [1650/7307]\tTime  0.024 ( 0.024)\tLoss 6.2742e-01 (6.2020e-01)\tAcc@1 0.98828125 (0.9935077225923683)\n",
            "Test: [1700/7307]\tTime  0.023 ( 0.024)\tLoss 6.1254e-01 (6.2023e-01)\tAcc@1 0.99609375 (0.993464322457378)\n",
            "Test: [1750/7307]\tTime  0.023 ( 0.024)\tLoss 6.1923e-01 (6.2028e-01)\tAcc@1 1.0 (0.9934590948029697)\n",
            "Test: [1800/7307]\tTime  0.024 ( 0.024)\tLoss 6.0441e-01 (6.2024e-01)\tAcc@1 0.99609375 (0.993462833148251)\n",
            "Test: [1850/7307]\tTime  0.024 ( 0.024)\tLoss 5.8928e-01 (6.2020e-01)\tAcc@1 1.0 (0.9935022454078877)\n",
            "Test: [1900/7307]\tTime  0.024 ( 0.024)\tLoss 6.2321e-01 (6.2021e-01)\tAcc@1 0.984375 (0.9935190360336665)\n",
            "Test: [1950/7307]\tTime  0.024 ( 0.024)\tLoss 6.1611e-01 (6.2020e-01)\tAcc@1 1.0 (0.9935129420809841)\n",
            "Test: [2000/7307]\tTime  0.024 ( 0.024)\tLoss 6.3067e-01 (6.2021e-01)\tAcc@1 0.984375 (0.9934778704397801)\n",
            "Test: [2050/7307]\tTime  0.023 ( 0.024)\tLoss 6.2480e-01 (6.2012e-01)\tAcc@1 0.98828125 (0.993463554363725)\n",
            "Test: [2100/7307]\tTime  0.024 ( 0.024)\tLoss 6.1934e-01 (6.2014e-01)\tAcc@1 0.9921875 (0.9934462012137077)\n",
            "Test: [2150/7307]\tTime  0.023 ( 0.024)\tLoss 6.1690e-01 (6.2013e-01)\tAcc@1 0.984375 (0.9934242067642957)\n",
            "Test: [2200/7307]\tTime  0.023 ( 0.024)\tLoss 6.1395e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9933978873239436)\n",
            "Test: [2250/7307]\tTime  0.024 ( 0.024)\tLoss 6.2365e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9933883551754775)\n",
            "Test: [2300/7307]\tTime  0.023 ( 0.024)\tLoss 6.3693e-01 (6.2017e-01)\tAcc@1 0.9921875 (0.9933962136027814)\n",
            "Test: [2350/7307]\tTime  0.026 ( 0.024)\tLoss 6.3460e-01 (6.2015e-01)\tAcc@1 0.98828125 (0.9933871225010634)\n",
            "Test: [2400/7307]\tTime  0.024 ( 0.024)\tLoss 6.1099e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9933800369637651)\n",
            "Test: [2450/7307]\tTime  0.025 ( 0.024)\tLoss 6.2889e-01 (6.2012e-01)\tAcc@1 0.9921875 (0.9933796154630763)\n",
            "Test: [2500/7307]\tTime  0.024 ( 0.024)\tLoss 6.3122e-01 (6.2017e-01)\tAcc@1 0.98828125 (0.9933792108156737)\n",
            "Test: [2550/7307]\tTime  0.023 ( 0.024)\tLoss 6.2587e-01 (6.2019e-01)\tAcc@1 0.99609375 (0.9934017909643277)\n",
            "Test: [2600/7307]\tTime  0.024 ( 0.024)\tLoss 6.0264e-01 (6.2016e-01)\tAcc@1 0.9921875 (0.9934129901960784)\n",
            "Test: [2650/7307]\tTime  0.024 ( 0.024)\tLoss 6.1341e-01 (6.2014e-01)\tAcc@1 0.9921875 (0.9934208199735949)\n",
            "Test: [2700/7307]\tTime  0.024 ( 0.024)\tLoss 6.1575e-01 (6.2012e-01)\tAcc@1 0.984375 (0.993416790077749)\n",
            "Test: [2750/7307]\tTime  0.023 ( 0.024)\tLoss 6.0666e-01 (6.2009e-01)\tAcc@1 0.9921875 (0.9934256861141403)\n",
            "Test: [2800/7307]\tTime  0.024 ( 0.024)\tLoss 6.1899e-01 (6.2013e-01)\tAcc@1 1.0 (0.9934175294537665)\n",
            "Test: [2850/7307]\tTime  0.023 ( 0.024)\tLoss 6.3403e-01 (6.2014e-01)\tAcc@1 0.9921875 (0.9934233602244826)\n",
            "Test: [2900/7307]\tTime  0.024 ( 0.024)\tLoss 6.2638e-01 (6.2010e-01)\tAcc@1 0.99609375 (0.9934330295587729)\n",
            "Test: [2950/7307]\tTime  0.024 ( 0.024)\tLoss 6.2044e-01 (6.2008e-01)\tAcc@1 1.0 (0.9934397238224331)\n",
            "Test: [3000/7307]\tTime  0.024 ( 0.024)\tLoss 6.1200e-01 (6.2001e-01)\tAcc@1 1.0 (0.9934487983172275)\n",
            "Test: [3050/7307]\tTime  0.024 ( 0.024)\tLoss 6.1544e-01 (6.2004e-01)\tAcc@1 0.99609375 (0.9934524541134054)\n",
            "Test: [3100/7307]\tTime  0.023 ( 0.024)\tLoss 6.2622e-01 (6.2006e-01)\tAcc@1 0.98828125 (0.9934219808126411)\n",
            "Test: [3150/7307]\tTime  0.025 ( 0.024)\tLoss 6.1818e-01 (6.2002e-01)\tAcc@1 0.98046875 (0.9934247064423992)\n",
            "Test: [3200/7307]\tTime  0.024 ( 0.024)\tLoss 6.2582e-01 (6.2002e-01)\tAcc@1 0.99609375 (0.9934322282099344)\n",
            "Test: [3250/7307]\tTime  0.024 ( 0.024)\tLoss 6.1544e-01 (6.2003e-01)\tAcc@1 0.98828125 (0.9934226968625038)\n",
            "Test: [3300/7307]\tTime  0.024 ( 0.024)\tLoss 6.2895e-01 (6.2003e-01)\tAcc@1 0.98828125 (0.9934075374886399)\n",
            "Test: [3350/7307]\tTime  0.024 ( 0.024)\tLoss 6.2340e-01 (6.2003e-01)\tAcc@1 0.99609375 (0.9933986589823933)\n",
            "Test: [3400/7307]\tTime  0.024 ( 0.024)\tLoss 6.1973e-01 (6.2002e-01)\tAcc@1 0.9921875 (0.9934118641576007)\n",
            "Test: [3450/7307]\tTime  0.024 ( 0.024)\tLoss 6.2417e-01 (6.2001e-01)\tAcc@1 0.9921875 (0.9934043121558969)\n",
            "Test: [3500/7307]\tTime  0.023 ( 0.024)\tLoss 6.3824e-01 (6.2001e-01)\tAcc@1 1.0 (0.993409249143102)\n",
            "Test: [3550/7307]\tTime  0.026 ( 0.024)\tLoss 6.1979e-01 (6.2001e-01)\tAcc@1 0.984375 (0.993395346381301)\n",
            "Test: [3600/7307]\tTime  0.023 ( 0.024)\tLoss 6.2713e-01 (6.2002e-01)\tAcc@1 0.9921875 (0.9933948469175229)\n",
            "Test: [3650/7307]\tTime  0.024 ( 0.024)\tLoss 6.1006e-01 (6.2004e-01)\tAcc@1 0.99609375 (0.9933836620104081)\n",
            "Test: [3700/7307]\tTime  0.024 ( 0.024)\tLoss 6.1841e-01 (6.2005e-01)\tAcc@1 0.99609375 (0.9933938884760876)\n",
            "Test: [3750/7307]\tTime  0.025 ( 0.024)\tLoss 6.0505e-01 (6.2004e-01)\tAcc@1 0.99609375 (0.9934028009197547)\n",
            "Test: [3800/7307]\tTime  0.024 ( 0.024)\tLoss 6.3205e-01 (6.2005e-01)\tAcc@1 0.99609375 (0.9934248388581952)\n",
            "Test: [3850/7307]\tTime  0.024 ( 0.024)\tLoss 6.2859e-01 (6.2004e-01)\tAcc@1 0.984375 (0.9934310893274474)\n",
            "Test: [3900/7307]\tTime  0.023 ( 0.024)\tLoss 6.2650e-01 (6.2006e-01)\tAcc@1 0.98828125 (0.9934091418866957)\n",
            "Test: [3950/7307]\tTime  0.024 ( 0.024)\tLoss 6.0878e-01 (6.2008e-01)\tAcc@1 1.0 (0.9934183988230828)\n",
            "Test: [4000/7307]\tTime  0.024 ( 0.024)\tLoss 6.3184e-01 (6.2009e-01)\tAcc@1 0.9921875 (0.9934147322544364)\n",
            "Test: [4050/7307]\tTime  0.024 ( 0.024)\tLoss 6.3379e-01 (6.2010e-01)\tAcc@1 1.0 (0.9934169418044927)\n",
            "Test: [4100/7307]\tTime  0.024 ( 0.024)\tLoss 6.0958e-01 (6.2008e-01)\tAcc@1 0.99609375 (0.993421955010973)\n",
            "Test: [4150/7307]\tTime  0.024 ( 0.024)\tLoss 6.1547e-01 (6.2008e-01)\tAcc@1 0.99609375 (0.9934249653697904)\n",
            "Test: [4200/7307]\tTime  0.024 ( 0.024)\tLoss 6.1156e-01 (6.2008e-01)\tAcc@1 0.984375 (0.9934111669840514)\n",
            "Test: [4250/7307]\tTime  0.023 ( 0.024)\tLoss 6.3083e-01 (6.2011e-01)\tAcc@1 1.0 (0.9934032065984474)\n",
            "Test: [4300/7307]\tTime  0.023 ( 0.024)\tLoss 6.3325e-01 (6.2011e-01)\tAcc@1 0.9921875 (0.9934072381422925)\n",
            "Test: [4350/7307]\tTime  0.025 ( 0.024)\tLoss 6.1638e-01 (6.2010e-01)\tAcc@1 0.9921875 (0.9934111770282693)\n",
            "Test: [4400/7307]\tTime  0.024 ( 0.024)\tLoss 6.1947e-01 (6.2010e-01)\tAcc@1 0.9921875 (0.993413251249716)\n",
            "Test: [4450/7307]\tTime  0.023 ( 0.024)\tLoss 6.2664e-01 (6.2011e-01)\tAcc@1 0.9921875 (0.9934214221523253)\n",
            "Test: [4500/7307]\tTime  0.023 ( 0.024)\tLoss 6.1263e-01 (6.2011e-01)\tAcc@1 0.99609375 (0.9934146578538102)\n",
            "Test: [4550/7307]\tTime  0.023 ( 0.024)\tLoss 6.1747e-01 (6.2013e-01)\tAcc@1 1.0 (0.9934166254669303)\n",
            "Test: [4600/7307]\tTime  0.024 ( 0.024)\tLoss 6.3680e-01 (6.2012e-01)\tAcc@1 0.99609375 (0.9934236443164529)\n",
            "Test: [4650/7307]\tTime  0.024 ( 0.024)\tLoss 5.9373e-01 (6.2011e-01)\tAcc@1 1.0 (0.9934296723822834)\n",
            "Test: [4700/7307]\tTime  0.024 ( 0.024)\tLoss 6.0249e-01 (6.2011e-01)\tAcc@1 0.98828125 (0.9934247699957456)\n",
            "Test: [4750/7307]\tTime  0.024 ( 0.024)\tLoss 6.1099e-01 (6.2012e-01)\tAcc@1 0.9921875 (0.9934183264049674)\n",
            "Test: [4800/7307]\tTime  0.026 ( 0.024)\tLoss 6.2936e-01 (6.2009e-01)\tAcc@1 1.0 (0.9934339851072693)\n",
            "Test: [4850/7307]\tTime  0.027 ( 0.024)\tLoss 6.2340e-01 (6.2008e-01)\tAcc@1 1.0 (0.9934404633065347)\n",
            "Test: [4900/7307]\tTime  0.024 ( 0.024)\tLoss 6.2879e-01 (6.2009e-01)\tAcc@1 0.9921875 (0.993429274637829)\n",
            "Test: [4950/7307]\tTime  0.023 ( 0.024)\tLoss 6.1544e-01 (6.2009e-01)\tAcc@1 0.9921875 (0.9934254127953949)\n",
            "Test: [5000/7307]\tTime  0.023 ( 0.024)\tLoss 6.2231e-01 (6.2014e-01)\tAcc@1 0.9921875 (0.993434125674865)\n",
            "Test: [5050/7307]\tTime  0.023 ( 0.024)\tLoss 6.2241e-01 (6.2015e-01)\tAcc@1 0.9921875 (0.9934341590774104)\n",
            "Test: [5100/7307]\tTime  0.024 ( 0.024)\tLoss 6.0943e-01 (6.2016e-01)\tAcc@1 0.99609375 (0.9934318944814742)\n",
            "Test: [5150/7307]\tTime  0.024 ( 0.024)\tLoss 6.1341e-01 (6.2017e-01)\tAcc@1 0.9921875 (0.9934327072413124)\n",
            "Test: [5200/7307]\tTime  0.024 ( 0.024)\tLoss 6.2583e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9934365086041146)\n",
            "Test: [5250/7307]\tTime  0.024 ( 0.024)\tLoss 6.3676e-01 (6.2016e-01)\tAcc@1 1.0 (0.9934469327270996)\n",
            "Test: [5300/7307]\tTime  0.024 ( 0.024)\tLoss 6.2137e-01 (6.2016e-01)\tAcc@1 0.99609375 (0.9934564233163554)\n",
            "Test: [5350/7307]\tTime  0.024 ( 0.024)\tLoss 6.2340e-01 (6.2015e-01)\tAcc@1 1.0 (0.9934540564847693)\n",
            "Test: [5400/7307]\tTime  0.024 ( 0.024)\tLoss 5.9883e-01 (6.2015e-01)\tAcc@1 0.98828125 (0.9934459475097204)\n",
            "Test: [5450/7307]\tTime  0.024 ( 0.024)\tLoss 6.1231e-01 (6.2016e-01)\tAcc@1 0.99609375 (0.9934473032471106)\n",
            "Test: [5500/7307]\tTime  0.024 ( 0.024)\tLoss 6.2192e-01 (6.2016e-01)\tAcc@1 1.0 (0.9934543151245229)\n",
            "Test: [5550/7307]\tTime  0.024 ( 0.024)\tLoss 6.2044e-01 (6.2019e-01)\tAcc@1 1.0 (0.993436571113313)\n",
            "Test: [5600/7307]\tTime  0.024 ( 0.024)\tLoss 6.2224e-01 (6.2020e-01)\tAcc@1 0.984375 (0.9934310000446349)\n",
            "Test: [5650/7307]\tTime  0.024 ( 0.024)\tLoss 6.2640e-01 (6.2021e-01)\tAcc@1 0.99609375 (0.9934352050522032)\n",
            "Test: [5700/7307]\tTime  0.025 ( 0.024)\tLoss 6.2741e-01 (6.2022e-01)\tAcc@1 0.984375 (0.9934345399929837)\n",
            "Test: [5750/7307]\tTime  0.024 ( 0.024)\tLoss 6.2479e-01 (6.2021e-01)\tAcc@1 0.9921875 (0.9934338864980004)\n",
            "Test: [5800/7307]\tTime  0.024 ( 0.024)\tLoss 6.1518e-01 (6.2022e-01)\tAcc@1 0.99609375 (0.9934312241423893)\n",
            "Test: [5850/7307]\tTime  0.023 ( 0.024)\tLoss 6.2397e-01 (6.2023e-01)\tAcc@1 1.0 (0.9934332806357887)\n",
            "Test: [5900/7307]\tTime  0.024 ( 0.024)\tLoss 6.2192e-01 (6.2021e-01)\tAcc@1 1.0 (0.9934313304948313)\n",
            "Test: [5950/7307]\tTime  0.024 ( 0.024)\tLoss 6.3627e-01 (6.2020e-01)\tAcc@1 0.98828125 (0.9934359771466981)\n",
            "Test: [6000/7307]\tTime  0.024 ( 0.024)\tLoss 6.1840e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.9934301314364272)\n",
            "Test: [6050/7307]\tTime  0.024 ( 0.024)\tLoss 6.2669e-01 (6.2020e-01)\tAcc@1 0.98828125 (0.9934243823334986)\n",
            "Test: [6100/7307]\tTime  0.025 ( 0.024)\tLoss 6.2435e-01 (6.2019e-01)\tAcc@1 0.99609375 (0.9934321730044255)\n",
            "Test: [6150/7307]\tTime  0.024 ( 0.024)\tLoss 6.3599e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.9934157047634531)\n",
            "Test: [6200/7307]\tTime  0.023 ( 0.024)\tLoss 6.3023e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.993419660135462)\n",
            "Test: [6250/7307]\tTime  0.025 ( 0.024)\tLoss 6.2785e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.993422927331627)\n",
            "Test: [6300/7307]\tTime  0.023 ( 0.024)\tLoss 6.2081e-01 (6.2022e-01)\tAcc@1 0.9921875 (0.9934292423821616)\n",
            "Test: [6350/7307]\tTime  0.024 ( 0.024)\tLoss 6.1895e-01 (6.2021e-01)\tAcc@1 0.99609375 (0.9934305375137774)\n",
            "Test: [6400/7307]\tTime  0.025 ( 0.024)\tLoss 6.2101e-01 (6.2019e-01)\tAcc@1 0.9921875 (0.993433643180753)\n",
            "Test: [6450/7307]\tTime  0.024 ( 0.024)\tLoss 6.2341e-01 (6.2017e-01)\tAcc@1 0.99609375 (0.993436700705317)\n",
            "Test: [6500/7307]\tTime  0.024 ( 0.024)\tLoss 6.1841e-01 (6.2017e-01)\tAcc@1 0.9921875 (0.9934361059836948)\n",
            "Test: [6550/7307]\tTime  0.024 ( 0.024)\tLoss 6.1794e-01 (6.2017e-01)\tAcc@1 1.0 (0.9934414831705083)\n",
            "Test: [6600/7307]\tTime  0.024 ( 0.024)\tLoss 6.1506e-01 (6.2018e-01)\tAcc@1 1.0 (0.9934444118315406)\n",
            "Test: [6650/7307]\tTime  0.025 ( 0.024)\tLoss 6.2378e-01 (6.2019e-01)\tAcc@1 0.98828125 (0.9934519950007518)\n",
            "Test: [6700/7307]\tTime  0.025 ( 0.024)\tLoss 6.2879e-01 (6.2019e-01)\tAcc@1 0.99609375 (0.9934483892329503)\n",
            "Test: [6750/7307]\tTime  0.024 ( 0.024)\tLoss 6.0881e-01 (6.2020e-01)\tAcc@1 0.98828125 (0.9934500444378611)\n",
            "Test: [6800/7307]\tTime  0.024 ( 0.024)\tLoss 6.2341e-01 (6.2022e-01)\tAcc@1 0.9921875 (0.993445357300397)\n",
            "Test: [6850/7307]\tTime  0.023 ( 0.024)\tLoss 6.1044e-01 (6.2022e-01)\tAcc@1 0.9921875 (0.9934475806451613)\n",
            "Test: [6900/7307]\tTime  0.025 ( 0.024)\tLoss 6.1197e-01 (6.2023e-01)\tAcc@1 0.99609375 (0.9934373188668308)\n",
            "Test: [6950/7307]\tTime  0.023 ( 0.024)\tLoss 6.1294e-01 (6.2022e-01)\tAcc@1 0.9921875 (0.9934418159257661)\n",
            "Test: [7000/7307]\tTime  0.024 ( 0.024)\tLoss 6.1747e-01 (6.2020e-01)\tAcc@1 1.0 (0.9934451328381659)\n",
            "Test: [7050/7307]\tTime  0.023 ( 0.024)\tLoss 6.2676e-01 (6.2021e-01)\tAcc@1 0.9921875 (0.9934389847184797)\n",
            "Test: [7100/7307]\tTime  0.024 ( 0.024)\tLoss 6.0646e-01 (6.2020e-01)\tAcc@1 0.98828125 (0.9934417247570765)\n",
            "Test: [7150/7307]\tTime  0.024 ( 0.024)\tLoss 6.2267e-01 (6.2021e-01)\tAcc@1 0.98046875 (0.9934487964969935)\n",
            "Test: [7200/7307]\tTime  0.024 ( 0.024)\tLoss 6.1387e-01 (6.2021e-01)\tAcc@1 0.99609375 (0.9934438359255658)\n",
            "Test: [7250/7307]\tTime  0.024 ( 0.024)\tLoss 6.2825e-01 (6.2020e-01)\tAcc@1 0.9921875 (0.99343625017239)\n",
            "Test: [7300/7307]\tTime  0.023 ( 0.024)\tLoss 6.1730e-01 (6.2020e-01)\tAcc@1 0.9921875 (0.993448029379537)\n",
            "Test: [7306/7307]\tTime  0.087 ( 0.024)\tLoss 6.1369e-01 (6.2020e-01)\tAcc@1 0.9869281045751634 (0.9934482373325906)\n",
            "/content/LilNetX/trainer.py:1008: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  'Net Bytes {bytes}'.format(bytes=bits//8000) if not conf_network['vanilla'] else ''+ \\\n",
            " * Acc@1 0.9934482373325906Net Bytes 10.0\n",
            "/content/LilNetX/trainer.py:1011: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self.wandb.log({\"loss_val\": losses.avg, \"top1_val\": top1.avg, \"net_bytes_val\": bits//8000, \\\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ac_bytes ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        f1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: net_bytes_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      prec_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    recall_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   roc_auc_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      top1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ac_bytes 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_bytes 57688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best_val_top1 0.94478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_train 0.57959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                f1_val 0.98364\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           final_bytes 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        final_val_top1 0.81298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_reg_train 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train 0.65996\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              loss_val 0.6202\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             net_bytes 46.98259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         net_bytes_val 10.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            prec_train 0.81628\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              prec_val 0.97947\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          recall_train 0.45184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            recall_val 0.98817\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         roc_auc_train 0.70392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           roc_auc_val 0.99794\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        sparse_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       sparse_discrete 0.92425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sparse_in_decoded 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_in_discrete 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_out_decoded 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   sparse_out_discrete 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  sparse_slice_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: sparse_slice_discrete 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               timings 183.69458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            top1_train 0.80099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              top1_val 0.99345\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold1/wandb/run-20221216_041330-c0ee252573fd6829c5a76dcdd33c55df/logs\u001b[0m\n",
            "Total time taken: 219.10s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "!python3 LilNetX/main.py --config LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_2.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ggV6mavIVlU",
        "outputId": "c4ba6997-8332-4b74-a5ab-4088ba09cacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config loaded from:  /content/LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_2.yaml\n",
            "{'gpu_names': 'Tesla T4', 'gpu_count': 1, 'CUDA_VISIBLE_DEVICES': 'NotSet', 'cudnn.enabled': True, 'cudnn.benchmark': True, 'cudnn.deterministic': False, 'cudnn.version': 7605}\n",
            "{'memory': 'svmem(total=27331215360, available=25792970752, percent=5.6, used=1242079232, free=18320211968, active=1774718976, inactive=6654132224, buffers=408715264, cached=7360208896, shared=1216512, slab=406065152)'}\n",
            "{'CPUs': '4'}\n",
            "Running evaluation only\n",
            "Changed random seed to 805354\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlfml\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold2/wandb/run-20221216_041712-c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "tcmalloc: large alloc 3283222528 bytes == 0x6f36000 @  0x7f3dae8911e7 0x7f3d4c3ae14e 0x7f3d4c406745 0x7f3d4c3b19c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4bddd1 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Trainset has 446372 bening Xis and 196892 injected Xis\n",
            "tcmalloc: large alloc 9546981376 bytes == 0x7f3aa9a8a000 @  0x7f3dae8911e7 0x7f3d4c3ae14e 0x7f3d4c406745 0x7f3d4c3b19c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4b8c8c 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Testset has 1494253 bening Xis and 376236 injected Xis\n",
            "convnetids var = 0.003906249999999999\n",
            "classname = Conv2d, fan = 800, boundary = 0.9730919862656238\n",
            "classname = Conv2d, fan = 1600, boundary = 0.6000000000000001\n",
            "classname = Linear, fan = 64, boundary = 4.424428900898053\n",
            "classname = Linear, fan = 1, boundary = 38.69502519453203\n",
            "Number of parameters: 1.3605 M\n",
            "Using /root/.cache/torch_extensions/py38_cu102 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu102/torchac_backend/build.ninja...\n",
            "Building extension module torchac_backend...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module torchac_backend...\n",
            "Checkpoint found, continuing training from epoch 4\n",
            "Changing random seed to 805354\n",
            "Test: [  0/503]\tTime  1.139 ( 1.139)\tLoss 6.1574e-01 (6.1574e-01)\tAcc@1 0.9921875 (0.9921875)\n",
            "Test: [ 50/503]\tTime  0.023 ( 0.046)\tLoss 6.1944e-01 (6.2277e-01)\tAcc@1 0.97265625 (0.9827665441176471)\n",
            "Test: [100/503]\tTime  0.025 ( 0.036)\tLoss 6.1703e-01 (6.2207e-01)\tAcc@1 0.984375 (0.9831373762376238)\n",
            "Test: [150/503]\tTime  0.024 ( 0.032)\tLoss 6.4283e-01 (6.2277e-01)\tAcc@1 0.984375 (0.9833919701986755)\n",
            "Test: [200/503]\tTime  0.024 ( 0.030)\tLoss 6.2584e-01 (6.2271e-01)\tAcc@1 0.98046875 (0.9831312189054726)\n",
            "Test: [250/503]\tTime  0.024 ( 0.029)\tLoss 6.2894e-01 (6.2311e-01)\tAcc@1 0.97265625 (0.9828342878486056)\n",
            "Test: [300/503]\tTime  0.025 ( 0.028)\tLoss 6.2299e-01 (6.2295e-01)\tAcc@1 0.984375 (0.9830642649501661)\n",
            "Test: [350/503]\tTime  0.024 ( 0.028)\tLoss 6.3596e-01 (6.2297e-01)\tAcc@1 0.97265625 (0.9831063034188035)\n",
            "Test: [400/503]\tTime  0.028 ( 0.027)\tLoss 6.1373e-01 (6.2282e-01)\tAcc@1 0.9765625 (0.9831963061097256)\n",
            "Test: [450/503]\tTime  0.024 ( 0.027)\tLoss 6.2773e-01 (6.2287e-01)\tAcc@1 0.97265625 (0.9833009977827051)\n",
            "Test: [500/503]\tTime  0.023 ( 0.027)\tLoss 6.3279e-01 (6.2302e-01)\tAcc@1 0.98046875 (0.9829871506986028)\n",
            "Test: [502/503]\tTime  0.081 ( 0.027)\tLoss 6.1049e-01 (6.2300e-01)\tAcc@1 0.9858156028368794 (0.9830007850574802)\n",
            "/content/LilNetX/trainer.py:1008: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  'Net Bytes {bytes}'.format(bytes=bits//8000) if not conf_network['vanilla'] else ''+ \\\n",
            " * Acc@1 0.9830007850574802Net Bytes 61.0\n",
            "/content/LilNetX/trainer.py:1011: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self.wandb.log({\"loss_val\": losses.avg, \"top1_val\": top1.avg, \"net_bytes_val\": bits//8000, \\\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ac_bytes ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        f1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: net_bytes_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      prec_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    recall_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   roc_auc_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      top1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ac_bytes 61\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_bytes 57688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best_val_top1 0.94478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_train 0.57959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                f1_val 0.96008\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           final_bytes 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        final_val_top1 0.81298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_reg_train 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train 0.65996\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              loss_val 0.623\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             net_bytes 46.98259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         net_bytes_val 61.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            prec_train 0.81628\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              prec_val 0.93733\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          recall_train 0.45184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            recall_val 0.9846\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         roc_auc_train 0.70392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           roc_auc_val 0.99734\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        sparse_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       sparse_discrete 0.92425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sparse_in_decoded 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_in_discrete 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_out_decoded 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   sparse_out_discrete 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  sparse_slice_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: sparse_slice_discrete 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               timings 183.69458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            top1_train 0.80099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              top1_val 0.983\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold2/wandb/run-20221216_041712-c0ee252573fd6829c5a76dcdd33c55df/logs\u001b[0m\n",
            "Total time taken: 61.14s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "!python3 LilNetX/main.py --config LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_3.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAFd8rA3IWJP",
        "outputId": "aee14041-53e5-449b-9c29-a15983e389e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config loaded from:  /content/LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_3.yaml\n",
            "{'gpu_names': 'Tesla T4', 'gpu_count': 1, 'CUDA_VISIBLE_DEVICES': 'NotSet', 'cudnn.enabled': True, 'cudnn.benchmark': True, 'cudnn.deterministic': False, 'cudnn.version': 7605}\n",
            "{'memory': 'svmem(total=27331215360, available=25792954368, percent=5.6, used=1242095616, free=18317258752, active=1792278528, inactive=6641258496, buffers=409128960, cached=7362732032, shared=1216512, slab=406093824)'}\n",
            "{'CPUs': '4'}\n",
            "Running evaluation only\n",
            "Changed random seed to 805354\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlfml\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold3/wandb/run-20221216_041816-c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "tcmalloc: large alloc 3283222528 bytes == 0x7e18000 @  0x7fcdc6eff1e7 0x7fcd64a1c14e 0x7fcd64a74745 0x7fcd64a1f9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4bddd1 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Trainset has 446372 bening Xis and 196892 injected Xis\n",
            "tcmalloc: large alloc 9546981376 bytes == 0x7fcade1a6000 @  0x7fcdc6eff1e7 0x7fcd64a1c14e 0x7fcd64a74745 0x7fcd64a1f9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4b8c8c 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Testset has 1494253 bening Xis and 376236 injected Xis\n",
            "convnetids var = 0.003906249999999999\n",
            "classname = Conv2d, fan = 800, boundary = 0.9730919862656238\n",
            "classname = Conv2d, fan = 1600, boundary = 0.6000000000000001\n",
            "classname = Linear, fan = 64, boundary = 4.424428900898053\n",
            "classname = Linear, fan = 1, boundary = 38.69502519453203\n",
            "Number of parameters: 1.3605 M\n",
            "Using /root/.cache/torch_extensions/py38_cu102 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu102/torchac_backend/build.ninja...\n",
            "Building extension module torchac_backend...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module torchac_backend...\n",
            "Checkpoint found, continuing training from epoch 10\n",
            "Changing random seed to 805354\n",
            "Test: [   0/7307]\tTime  0.623 ( 0.623)\tLoss 6.2096e-01 (6.2096e-01)\tAcc@1 0.98828125 (0.98828125)\n",
            "Test: [  50/7307]\tTime  0.025 ( 0.036)\tLoss 6.2693e-01 (6.2039e-01)\tAcc@1 1.0 (0.9935661764705882)\n",
            "Test: [ 100/7307]\tTime  0.024 ( 0.030)\tLoss 6.1998e-01 (6.2009e-01)\tAcc@1 0.9921875 (0.9936571782178217)\n",
            "Test: [ 150/7307]\tTime  0.024 ( 0.028)\tLoss 6.1450e-01 (6.2011e-01)\tAcc@1 0.99609375 (0.9936103062913907)\n",
            "Test: [ 200/7307]\tTime  0.024 ( 0.027)\tLoss 6.0263e-01 (6.2009e-01)\tAcc@1 1.0 (0.9933924129353234)\n",
            "Test: [ 250/7307]\tTime  0.025 ( 0.027)\tLoss 6.1635e-01 (6.1929e-01)\tAcc@1 0.98828125 (0.9935103336653387)\n",
            "Test: [ 300/7307]\tTime  0.024 ( 0.026)\tLoss 6.0489e-01 (6.1948e-01)\tAcc@1 0.9921875 (0.9934593023255814)\n",
            "Test: [ 350/7307]\tTime  0.024 ( 0.026)\tLoss 6.1895e-01 (6.1969e-01)\tAcc@1 1.0 (0.993400551994302)\n",
            "Test: [ 400/7307]\tTime  0.024 ( 0.026)\tLoss 6.1454e-01 (6.1985e-01)\tAcc@1 0.9921875 (0.9932687811720698)\n",
            "Test: [ 450/7307]\tTime  0.025 ( 0.026)\tLoss 6.2062e-01 (6.2011e-01)\tAcc@1 0.98828125 (0.993166227827051)\n",
            "Test: [ 500/7307]\tTime  0.024 ( 0.025)\tLoss 6.2965e-01 (6.2019e-01)\tAcc@1 0.99609375 (0.993123128742515)\n",
            "Test: [ 550/7307]\tTime  0.025 ( 0.025)\tLoss 6.0708e-01 (6.2009e-01)\tAcc@1 0.99609375 (0.9932579968239564)\n",
            "Test: [ 600/7307]\tTime  0.024 ( 0.025)\tLoss 6.1938e-01 (6.2021e-01)\tAcc@1 0.98828125 (0.9932014351081531)\n",
            "Test: [ 650/7307]\tTime  0.024 ( 0.025)\tLoss 6.2236e-01 (6.2025e-01)\tAcc@1 0.9921875 (0.9932795698924731)\n",
            "Test: [ 700/7307]\tTime  0.023 ( 0.025)\tLoss 6.2309e-01 (6.2031e-01)\tAcc@1 0.98828125 (0.9932295381597718)\n",
            "Test: [ 750/7307]\tTime  0.024 ( 0.025)\tLoss 6.1642e-01 (6.2015e-01)\tAcc@1 0.9921875 (0.9932173768308922)\n",
            "Test: [ 800/7307]\tTime  0.024 ( 0.025)\tLoss 6.1416e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9931774734706617)\n",
            "Test: [ 850/7307]\tTime  0.024 ( 0.025)\tLoss 6.1709e-01 (6.1999e-01)\tAcc@1 0.99609375 (0.9932386530552292)\n",
            "Test: [ 950/7307]\tTime  0.024 ( 0.025)\tLoss 6.1034e-01 (6.2001e-01)\tAcc@1 0.98828125 (0.9932924224500526)\n",
            "Test: [1000/7307]\tTime  0.026 ( 0.025)\tLoss 6.3030e-01 (6.2006e-01)\tAcc@1 0.99609375 (0.9933308878621379)\n",
            "Test: [1050/7307]\tTime  0.023 ( 0.025)\tLoss 6.1895e-01 (6.2007e-01)\tAcc@1 0.99609375 (0.9933582599904852)\n",
            "Test: [1100/7307]\tTime  0.024 ( 0.025)\tLoss 6.2654e-01 (6.2002e-01)\tAcc@1 0.98828125 (0.9933370231607629)\n",
            "Test: [1150/7307]\tTime  0.024 ( 0.025)\tLoss 6.1989e-01 (6.1995e-01)\tAcc@1 0.99609375 (0.9933379941355344)\n",
            "Test: [1200/7307]\tTime  0.026 ( 0.025)\tLoss 6.1445e-01 (6.1997e-01)\tAcc@1 0.98828125 (0.9932998542880933)\n",
            "Test: [1250/7307]\tTime  0.024 ( 0.025)\tLoss 6.2588e-01 (6.1995e-01)\tAcc@1 0.99609375 (0.9932834982014388)\n",
            "Test: [1300/7307]\tTime  0.024 ( 0.025)\tLoss 5.9541e-01 (6.2010e-01)\tAcc@1 0.9921875 (0.9932864142966948)\n",
            "Test: [1350/7307]\tTime  0.024 ( 0.025)\tLoss 6.3083e-01 (6.2023e-01)\tAcc@1 1.0 (0.9932804404145078)\n",
            "Test: [1400/7307]\tTime  0.024 ( 0.025)\tLoss 6.2637e-01 (6.2022e-01)\tAcc@1 1.0 (0.993316715738758)\n",
            "Test: [1450/7307]\tTime  0.025 ( 0.025)\tLoss 6.2498e-01 (6.2021e-01)\tAcc@1 0.99609375 (0.9933128015161957)\n",
            "Test: [1500/7307]\tTime  0.024 ( 0.025)\tLoss 6.3150e-01 (6.2017e-01)\tAcc@1 0.99609375 (0.993340377248501)\n",
            "Test: [1550/7307]\tTime  0.024 ( 0.025)\tLoss 6.2434e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.9933460267569311)\n",
            "Test: [1600/7307]\tTime  0.024 ( 0.025)\tLoss 6.2785e-01 (6.2021e-01)\tAcc@1 0.99609375 (0.9933928013741412)\n",
            "Test: [1650/7307]\tTime  0.025 ( 0.025)\tLoss 6.2578e-01 (6.2023e-01)\tAcc@1 0.9921875 (0.9934249129315567)\n",
            "Test: [1700/7307]\tTime  0.025 ( 0.025)\tLoss 6.1229e-01 (6.2025e-01)\tAcc@1 0.99609375 (0.99340920781893)\n",
            "Test: [1750/7307]\tTime  0.023 ( 0.025)\tLoss 6.1909e-01 (6.2029e-01)\tAcc@1 1.0 (0.9934211700456882)\n",
            "Test: [1800/7307]\tTime  0.025 ( 0.025)\tLoss 6.0607e-01 (6.2026e-01)\tAcc@1 0.9921875 (0.9934237923375903)\n",
            "Test: [1850/7307]\tTime  0.024 ( 0.025)\tLoss 5.8929e-01 (6.2022e-01)\tAcc@1 1.0 (0.9934515971096705)\n",
            "Test: [1900/7307]\tTime  0.024 ( 0.025)\tLoss 6.2084e-01 (6.2023e-01)\tAcc@1 0.98828125 (0.9934758844029458)\n",
            "Test: [1950/7307]\tTime  0.024 ( 0.024)\tLoss 6.1747e-01 (6.2021e-01)\tAcc@1 0.99609375 (0.9934809072270631)\n",
            "Test: [2000/7307]\tTime  0.024 ( 0.024)\tLoss 6.2827e-01 (6.2021e-01)\tAcc@1 0.98828125 (0.9934798225887056)\n",
            "Test: [2050/7307]\tTime  0.023 ( 0.024)\tLoss 6.2381e-01 (6.2012e-01)\tAcc@1 0.9921875 (0.993452127011214)\n",
            "Test: [2100/7307]\tTime  0.024 ( 0.024)\tLoss 6.1703e-01 (6.2014e-01)\tAcc@1 0.99609375 (0.9934369050452165)\n",
            "Test: [2150/7307]\tTime  0.024 ( 0.024)\tLoss 6.1642e-01 (6.2013e-01)\tAcc@1 0.98828125 (0.9934169427010693)\n",
            "Test: [2200/7307]\tTime  0.024 ( 0.024)\tLoss 6.1638e-01 (6.2015e-01)\tAcc@1 0.9921875 (0.9934032116083599)\n",
            "Test: [2250/7307]\tTime  0.024 ( 0.024)\tLoss 6.2291e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9933866198356286)\n",
            "Test: [2300/7307]\tTime  0.024 ( 0.024)\tLoss 6.3714e-01 (6.2017e-01)\tAcc@1 0.9921875 (0.9934063993915688)\n",
            "Test: [2350/7307]\tTime  0.023 ( 0.024)\tLoss 6.3566e-01 (6.2016e-01)\tAcc@1 0.984375 (0.993382137920034)\n",
            "Test: [2400/7307]\tTime  0.024 ( 0.024)\tLoss 6.1105e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9934044408579759)\n",
            "Test: [2450/7307]\tTime  0.023 ( 0.024)\tLoss 6.3051e-01 (6.2011e-01)\tAcc@1 0.984375 (0.9934035215218279)\n",
            "Test: [2500/7307]\tTime  0.024 ( 0.024)\tLoss 6.3123e-01 (6.2016e-01)\tAcc@1 0.98828125 (0.9933979533186725)\n",
            "Test: [2550/7307]\tTime  0.024 ( 0.024)\tLoss 6.2509e-01 (6.2019e-01)\tAcc@1 1.0 (0.9934125098000784)\n",
            "Test: [2600/7307]\tTime  0.024 ( 0.024)\tLoss 6.0114e-01 (6.2016e-01)\tAcc@1 0.99609375 (0.9934204993271819)\n",
            "Test: [2650/7307]\tTime  0.024 ( 0.024)\tLoss 6.1721e-01 (6.2015e-01)\tAcc@1 0.984375 (0.9934222934741607)\n",
            "Test: [2700/7307]\tTime  0.024 ( 0.024)\tLoss 6.1590e-01 (6.2012e-01)\tAcc@1 0.984375 (0.9934182363013698)\n",
            "Test: [2750/7307]\tTime  0.024 ( 0.024)\tLoss 6.0557e-01 (6.2009e-01)\tAcc@1 0.9921875 (0.9934285259905489)\n",
            "Test: [2800/7307]\tTime  0.024 ( 0.024)\tLoss 6.1896e-01 (6.2013e-01)\tAcc@1 1.0 (0.9934286861835059)\n",
            "Test: [2850/7307]\tTime  0.024 ( 0.024)\tLoss 6.3160e-01 (6.2013e-01)\tAcc@1 0.99609375 (0.9934452823570677)\n",
            "Test: [2900/7307]\tTime  0.029 ( 0.024)\tLoss 6.2637e-01 (6.2010e-01)\tAcc@1 0.99609375 (0.9934397621509824)\n",
            "Test: [2950/7307]\tTime  0.024 ( 0.024)\tLoss 6.2127e-01 (6.2007e-01)\tAcc@1 0.99609375 (0.9934489897492376)\n",
            "Test: [3000/7307]\tTime  0.024 ( 0.024)\tLoss 6.1175e-01 (6.2001e-01)\tAcc@1 1.0 (0.993459211512829)\n",
            "Test: [3050/7307]\tTime  0.025 ( 0.024)\tLoss 6.1698e-01 (6.2003e-01)\tAcc@1 0.9921875 (0.9934614163389053)\n",
            "Test: [3100/7307]\tTime  0.024 ( 0.024)\tLoss 6.2380e-01 (6.2005e-01)\tAcc@1 0.9921875 (0.9934307985327314)\n",
            "Test: [3150/7307]\tTime  0.025 ( 0.024)\tLoss 6.1779e-01 (6.2000e-01)\tAcc@1 0.98046875 (0.9934433017296096)\n",
            "Test: [3200/7307]\tTime  0.024 ( 0.024)\tLoss 6.2599e-01 (6.2000e-01)\tAcc@1 0.99609375 (0.9934651768978444)\n",
            "Test: [3250/7307]\tTime  0.024 ( 0.024)\tLoss 6.1532e-01 (6.2001e-01)\tAcc@1 0.98828125 (0.9934515341433405)\n",
            "Test: [3300/7307]\tTime  0.024 ( 0.024)\tLoss 6.3260e-01 (6.2001e-01)\tAcc@1 0.98046875 (0.9934359379733414)\n",
            "Test: [3350/7307]\tTime  0.024 ( 0.024)\tLoss 6.2565e-01 (6.2002e-01)\tAcc@1 0.9921875 (0.9934208072217249)\n",
            "Test: [3400/7307]\tTime  0.024 ( 0.024)\tLoss 6.1933e-01 (6.2001e-01)\tAcc@1 0.9921875 (0.9934233497500735)\n",
            "Test: [3450/7307]\tTime  0.024 ( 0.024)\tLoss 6.2286e-01 (6.2000e-01)\tAcc@1 0.99609375 (0.9934246866850188)\n",
            "Test: [3500/7307]\tTime  0.024 ( 0.024)\tLoss 6.3824e-01 (6.1999e-01)\tAcc@1 1.0 (0.9934170594115966)\n",
            "Test: [3550/7307]\tTime  0.026 ( 0.024)\tLoss 6.1988e-01 (6.1999e-01)\tAcc@1 0.984375 (0.993404146719234)\n",
            "Test: [3600/7307]\tTime  0.024 ( 0.024)\tLoss 6.2718e-01 (6.2001e-01)\tAcc@1 0.9921875 (0.9933970164537629)\n",
            "Test: [3650/7307]\tTime  0.024 ( 0.024)\tLoss 6.1005e-01 (6.2003e-01)\tAcc@1 0.99609375 (0.993379382360997)\n",
            "Test: [3700/7307]\tTime  0.024 ( 0.024)\tLoss 6.1840e-01 (6.2005e-01)\tAcc@1 0.99609375 (0.993388611186166)\n",
            "Test: [3750/7307]\tTime  0.023 ( 0.024)\tLoss 6.0653e-01 (6.2004e-01)\tAcc@1 0.9921875 (0.9933892628632365)\n",
            "Test: [3800/7307]\tTime  0.023 ( 0.024)\tLoss 6.3176e-01 (6.2005e-01)\tAcc@1 0.99609375 (0.9934145619573796)\n",
            "Test: [3850/7307]\tTime  0.024 ( 0.024)\tLoss 6.2456e-01 (6.2002e-01)\tAcc@1 0.99609375 (0.9934412327966762)\n",
            "Test: [3900/7307]\tTime  0.024 ( 0.024)\tLoss 6.2685e-01 (6.2004e-01)\tAcc@1 0.984375 (0.9934331741861061)\n",
            "Test: [3950/7307]\tTime  0.024 ( 0.024)\tLoss 6.0857e-01 (6.2006e-01)\tAcc@1 1.0 (0.9934520137307011)\n",
            "Test: [4000/7307]\tTime  0.024 ( 0.024)\tLoss 6.2787e-01 (6.2007e-01)\tAcc@1 1.0 (0.9934401165333666)\n",
            "Test: [4050/7307]\tTime  0.024 ( 0.024)\tLoss 6.3380e-01 (6.2008e-01)\tAcc@1 1.0 (0.9934439413107875)\n",
            "Test: [4100/7307]\tTime  0.024 ( 0.024)\tLoss 6.0855e-01 (6.2007e-01)\tAcc@1 0.99609375 (0.9934371951962936)\n",
            "Test: [4150/7307]\tTime  0.024 ( 0.024)\tLoss 6.1693e-01 (6.2006e-01)\tAcc@1 0.9921875 (0.9934371988677427)\n",
            "Test: [4200/7307]\tTime  0.024 ( 0.024)\tLoss 6.0891e-01 (6.2006e-01)\tAcc@1 0.9921875 (0.993429763746727)\n",
            "Test: [4250/7307]\tTime  0.024 ( 0.024)\tLoss 6.3115e-01 (6.2009e-01)\tAcc@1 1.0 (0.9934179090214067)\n",
            "Test: [4300/7307]\tTime  0.024 ( 0.024)\tLoss 6.3716e-01 (6.2009e-01)\tAcc@1 0.984375 (0.9934199532085561)\n",
            "Test: [4350/7307]\tTime  0.024 ( 0.024)\tLoss 6.1637e-01 (6.2008e-01)\tAcc@1 0.9921875 (0.9934264393242933)\n",
            "Test: [4400/7307]\tTime  0.024 ( 0.024)\tLoss 6.1841e-01 (6.2008e-01)\tAcc@1 0.99609375 (0.9934265649852306)\n",
            "Test: [4450/7307]\tTime  0.024 ( 0.024)\tLoss 6.2538e-01 (6.2010e-01)\tAcc@1 0.99609375 (0.9934266878229612)\n",
            "Test: [4500/7307]\tTime  0.026 ( 0.024)\tLoss 6.1544e-01 (6.2010e-01)\tAcc@1 0.98828125 (0.9934285436569651)\n",
            "Test: [4550/7307]\tTime  0.024 ( 0.024)\tLoss 6.1897e-01 (6.2011e-01)\tAcc@1 0.99609375 (0.993430358712371)\n",
            "Test: [4600/7307]\tTime  0.024 ( 0.024)\tLoss 6.3916e-01 (6.2010e-01)\tAcc@1 0.9921875 (0.9934372283199304)\n",
            "Test: [4650/7307]\tTime  0.027 ( 0.024)\tLoss 5.9378e-01 (6.2009e-01)\tAcc@1 1.0 (0.9934481495914858)\n",
            "Test: [4700/7307]\tTime  0.024 ( 0.024)\tLoss 6.0248e-01 (6.2010e-01)\tAcc@1 0.98828125 (0.9934380650393533)\n",
            "Test: [4750/7307]\tTime  0.024 ( 0.024)\tLoss 6.1243e-01 (6.2011e-01)\tAcc@1 0.98828125 (0.993425726162913)\n",
            "Test: [4800/7307]\tTime  0.024 ( 0.024)\tLoss 6.3176e-01 (6.2008e-01)\tAcc@1 0.99609375 (0.9934331714746928)\n",
            "Test: [4850/7307]\tTime  0.024 ( 0.024)\tLoss 6.2582e-01 (6.2007e-01)\tAcc@1 0.99609375 (0.99343643707483)\n",
            "Test: [4900/7307]\tTime  0.024 ( 0.024)\tLoss 6.2879e-01 (6.2008e-01)\tAcc@1 0.9921875 (0.9934260865129565)\n",
            "Test: [4950/7307]\tTime  0.023 ( 0.024)\tLoss 6.1544e-01 (6.2009e-01)\tAcc@1 0.9921875 (0.9934120001009897)\n",
            "Test: [5000/7307]\tTime  0.024 ( 0.024)\tLoss 6.2231e-01 (6.2013e-01)\tAcc@1 0.9921875 (0.9934106928614277)\n",
            "Test: [5050/7307]\tTime  0.025 ( 0.024)\tLoss 6.2380e-01 (6.2014e-01)\tAcc@1 0.9921875 (0.9934155983963572)\n",
            "Test: [5100/7307]\tTime  0.024 ( 0.024)\tLoss 6.0945e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9934119841697706)\n",
            "Test: [5150/7307]\tTime  0.023 ( 0.024)\tLoss 6.1338e-01 (6.2016e-01)\tAcc@1 0.9921875 (0.9934129901960784)\n",
            "Test: [5200/7307]\tTime  0.024 ( 0.024)\tLoss 6.2619e-01 (6.2015e-01)\tAcc@1 0.99609375 (0.9934139768794462)\n",
            "Test: [5250/7307]\tTime  0.024 ( 0.024)\tLoss 6.3676e-01 (6.2016e-01)\tAcc@1 1.0 (0.993417920396115)\n",
            "Test: [5300/7307]\tTime  0.024 ( 0.024)\tLoss 6.2137e-01 (6.2016e-01)\tAcc@1 0.99609375 (0.9934328428598378)\n",
            "Test: [5350/7307]\tTime  0.024 ( 0.024)\tLoss 6.2341e-01 (6.2015e-01)\tAcc@1 1.0 (0.9934241263315268)\n",
            "Test: [5400/7307]\tTime  0.026 ( 0.024)\tLoss 5.9822e-01 (6.2015e-01)\tAcc@1 0.98828125 (0.9934199106646917)\n",
            "Test: [5450/7307]\tTime  0.024 ( 0.024)\tLoss 6.1262e-01 (6.2016e-01)\tAcc@1 0.99609375 (0.9934200720051367)\n",
            "Test: [5500/7307]\tTime  0.024 ( 0.024)\tLoss 6.2192e-01 (6.2016e-01)\tAcc@1 1.0 (0.9934386929649155)\n",
            "Test: [5550/7307]\tTime  0.024 ( 0.024)\tLoss 6.2192e-01 (6.2018e-01)\tAcc@1 0.99609375 (0.9934337563051703)\n",
            "Test: [5600/7307]\tTime  0.024 ( 0.024)\tLoss 6.2326e-01 (6.2018e-01)\tAcc@1 0.984375 (0.9934303026245314)\n",
            "Test: [5650/7307]\tTime  0.023 ( 0.024)\tLoss 6.2584e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.9934234538134843)\n",
            "Test: [5700/7307]\tTime  0.023 ( 0.024)\tLoss 6.2676e-01 (6.2020e-01)\tAcc@1 0.98828125 (0.9934283733116998)\n",
            "Test: [5750/7307]\tTime  0.024 ( 0.024)\tLoss 6.2435e-01 (6.2020e-01)\tAcc@1 0.9921875 (0.9934250565119109)\n",
            "Test: [5800/7307]\tTime  0.024 ( 0.024)\tLoss 6.1477e-01 (6.2021e-01)\tAcc@1 0.99609375 (0.9934258371401482)\n",
            "Test: [5850/7307]\tTime  0.023 ( 0.024)\tLoss 6.2342e-01 (6.2022e-01)\tAcc@1 1.0 (0.9934259368056743)\n",
            "Test: [5900/7307]\tTime  0.023 ( 0.024)\tLoss 6.2425e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.9934214010337231)\n",
            "Test: [5950/7307]\tTime  0.024 ( 0.024)\tLoss 6.3565e-01 (6.2019e-01)\tAcc@1 0.98828125 (0.9934195670895648)\n",
            "Test: [6000/7307]\tTime  0.024 ( 0.024)\tLoss 6.1613e-01 (6.2019e-01)\tAcc@1 1.0 (0.9934099525079153)\n",
            "Test: [6050/7307]\tTime  0.024 ( 0.024)\tLoss 6.2822e-01 (6.2019e-01)\tAcc@1 0.984375 (0.9934011423731615)\n",
            "Test: [6100/7307]\tTime  0.025 ( 0.024)\tLoss 6.2822e-01 (6.2019e-01)\tAcc@1 0.98828125 (0.9934014403376495)\n",
            "Test: [6150/7307]\tTime  0.024 ( 0.024)\tLoss 6.3492e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.993382681677776)\n",
            "Test: [6200/7307]\tTime  0.024 ( 0.024)\tLoss 6.3250e-01 (6.2020e-01)\tAcc@1 0.98828125 (0.993388163199484)\n",
            "Test: [6250/7307]\tTime  0.024 ( 0.024)\tLoss 6.2735e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.993391057430811)\n",
            "Test: [6300/7307]\tTime  0.024 ( 0.024)\tLoss 6.2083e-01 (6.2022e-01)\tAcc@1 0.9921875 (0.9933951456118076)\n",
            "Test: [6350/7307]\tTime  0.025 ( 0.024)\tLoss 6.1810e-01 (6.2020e-01)\tAcc@1 1.0 (0.9934010146039993)\n",
            "Test: [6400/7307]\tTime  0.024 ( 0.024)\tLoss 6.2088e-01 (6.2018e-01)\tAcc@1 0.9921875 (0.9934019098578347)\n",
            "Test: [6450/7307]\tTime  0.023 ( 0.024)\tLoss 6.2340e-01 (6.2017e-01)\tAcc@1 0.99609375 (0.9934088464966672)\n",
            "Test: [6500/7307]\tTime  0.024 ( 0.024)\tLoss 6.2063e-01 (6.2016e-01)\tAcc@1 0.98828125 (0.9934018564451623)\n",
            "Test: [6550/7307]\tTime  0.024 ( 0.024)\tLoss 6.1773e-01 (6.2017e-01)\tAcc@1 1.0 (0.9934068987559151)\n",
            "Test: [6600/7307]\tTime  0.025 ( 0.024)\tLoss 6.1692e-01 (6.2018e-01)\tAcc@1 0.99609375 (0.993411864679594)\n",
            "Test: [6650/7307]\tTime  0.026 ( 0.024)\tLoss 6.2329e-01 (6.2019e-01)\tAcc@1 0.98828125 (0.9934079461735077)\n",
            "Test: [6700/7307]\tTime  0.025 ( 0.024)\tLoss 6.2873e-01 (6.2019e-01)\tAcc@1 0.99609375 (0.9934058349500074)\n",
            "Test: [6750/7307]\tTime  0.025 ( 0.024)\tLoss 6.0709e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.9934083839431196)\n",
            "Test: [6800/7307]\tTime  0.026 ( 0.024)\tLoss 6.2192e-01 (6.2022e-01)\tAcc@1 0.99609375 (0.9934120441846788)\n",
            "Test: [6850/7307]\tTime  0.025 ( 0.024)\tLoss 6.0985e-01 (6.2021e-01)\tAcc@1 0.9921875 (0.9934116597941907)\n",
            "Test: [6900/7307]\tTime  0.024 ( 0.024)\tLoss 6.1166e-01 (6.2022e-01)\tAcc@1 0.99609375 (0.993410148891465)\n",
            "Test: [6950/7307]\tTime  0.023 ( 0.024)\tLoss 6.1156e-01 (6.2021e-01)\tAcc@1 0.99609375 (0.9934142794202273)\n",
            "Test: [7000/7307]\tTime  0.025 ( 0.024)\tLoss 6.1747e-01 (6.2019e-01)\tAcc@1 1.0 (0.9934166770818454)\n",
            "Test: [7050/7307]\tTime  0.024 ( 0.024)\tLoss 6.2601e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.9934146087434407)\n",
            "Test: [7100/7307]\tTime  0.024 ( 0.024)\tLoss 6.0409e-01 (6.2019e-01)\tAcc@1 0.9921875 (0.9934158701239262)\n",
            "Test: [7150/7307]\tTime  0.025 ( 0.024)\tLoss 6.1917e-01 (6.2020e-01)\tAcc@1 0.98828125 (0.9934209376311005)\n",
            "Test: [7200/7307]\tTime  0.024 ( 0.024)\tLoss 6.1380e-01 (6.2020e-01)\tAcc@1 0.99609375 (0.9934188827940564)\n",
            "Test: [7250/7307]\tTime  0.024 ( 0.024)\tLoss 6.2825e-01 (6.2019e-01)\tAcc@1 0.9921875 (0.9934109303889119)\n",
            "Test: [7300/7307]\tTime  0.023 ( 0.024)\tLoss 6.1558e-01 (6.2019e-01)\tAcc@1 0.99609375 (0.9934175327009999)\n",
            "Test: [7306/7307]\tTime  0.087 ( 0.024)\tLoss 6.1311e-01 (6.2019e-01)\tAcc@1 0.9869281045751634 (0.9934188332569719)\n",
            "/content/LilNetX/trainer.py:1008: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  'Net Bytes {bytes}'.format(bytes=bits//8000) if not conf_network['vanilla'] else ''+ \\\n",
            " * Acc@1 0.9934188332569719Net Bytes 12.0\n",
            "/content/LilNetX/trainer.py:1011: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self.wandb.log({\"loss_val\": losses.avg, \"top1_val\": top1.avg, \"net_bytes_val\": bits//8000, \\\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ac_bytes ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        f1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: net_bytes_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      prec_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    recall_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   roc_auc_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      top1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ac_bytes 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_bytes 57688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best_val_top1 0.94478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_train 0.57959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                f1_val 0.98355\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           final_bytes 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        final_val_top1 0.81298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_reg_train 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train 0.65996\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              loss_val 0.62019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             net_bytes 46.98259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         net_bytes_val 12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            prec_train 0.81628\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              prec_val 0.9801\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          recall_train 0.45184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            recall_val 0.98735\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         roc_auc_train 0.70392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           roc_auc_val 0.99804\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        sparse_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       sparse_discrete 0.92425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sparse_in_decoded 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_in_discrete 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_out_decoded 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   sparse_out_discrete 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  sparse_slice_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: sparse_slice_discrete 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               timings 183.69458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            top1_train 0.80099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              top1_val 0.99342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold3/wandb/run-20221216_041816-c0ee252573fd6829c5a76dcdd33c55df/logs\u001b[0m\n",
            "Total time taken: 220.12s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "!python3 LilNetX/main.py --config LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_4.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulz-btATIXdC",
        "outputId": "86925d66-db04-4f4b-e66c-d69bd25d5998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config loaded from:  /content/LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_4.yaml\n",
            "{'gpu_names': 'Tesla T4', 'gpu_count': 1, 'CUDA_VISIBLE_DEVICES': 'NotSet', 'cudnn.enabled': True, 'cudnn.benchmark': True, 'cudnn.deterministic': False, 'cudnn.version': 7605}\n",
            "{'memory': 'svmem(total=27331215360, available=25799712768, percent=5.6, used=1239564288, free=18326487040, active=1809793024, inactive=6617395200, buffers=410062848, cached=7355101184, shared=1216512, slab=406253568)'}\n",
            "{'CPUs': '4'}\n",
            "Running evaluation only\n",
            "Changed random seed to 805354\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlfml\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold4/wandb/run-20221216_042200-c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Resuming run \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "tcmalloc: large alloc 3283222528 bytes == 0x6fbe000 @  0x7f5518bbf1e7 0x7f54b66dc14e 0x7f54b6734745 0x7f54b66df9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4bddd1 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Trainset has 446372 bening Xis and 196892 injected Xis\n",
            "tcmalloc: large alloc 9546981376 bytes == 0x7f522fe8a000 @  0x7f5518bbf1e7 0x7f54b66dc14e 0x7f54b6734745 0x7f54b66df9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4b8c8c 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Testset has 1494253 bening Xis and 376236 injected Xis\n",
            "convnetids var = 0.003906249999999999\n",
            "classname = Conv2d, fan = 800, boundary = 0.9730919862656238\n",
            "classname = Conv2d, fan = 1600, boundary = 0.6000000000000001\n",
            "classname = Linear, fan = 64, boundary = 4.424428900898053\n",
            "classname = Linear, fan = 1, boundary = 38.69502519453203\n",
            "Number of parameters: 1.3605 M\n",
            "Using /root/.cache/torch_extensions/py38_cu102 as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/py38_cu102/torchac_backend/build.ninja...\n",
            "Building extension module torchac_backend...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "ninja: no work to do.\n",
            "Loading extension module torchac_backend...\n",
            "Checkpoint found, continuing training from epoch 10\n",
            "Changing random seed to 805354\n",
            "Test: [   0/7307]\tTime  0.576 ( 0.576)\tLoss 6.2084e-01 (6.2084e-01)\tAcc@1 0.98828125 (0.98828125)\n",
            "Test: [  50/7307]\tTime  0.024 ( 0.035)\tLoss 6.2637e-01 (6.2081e-01)\tAcc@1 1.0 (0.9928002450980392)\n",
            "Test: [ 100/7307]\tTime  0.024 ( 0.029)\tLoss 6.1895e-01 (6.2036e-01)\tAcc@1 0.99609375 (0.9933090965346535)\n",
            "Test: [ 150/7307]\tTime  0.026 ( 0.027)\tLoss 6.1845e-01 (6.2045e-01)\tAcc@1 0.9921875 (0.9930929221854304)\n",
            "Test: [ 200/7307]\tTime  0.024 ( 0.027)\tLoss 6.0263e-01 (6.2044e-01)\tAcc@1 1.0 (0.9929065609452736)\n",
            "Test: [ 250/7307]\tTime  0.026 ( 0.026)\tLoss 6.1680e-01 (6.1964e-01)\tAcc@1 0.98828125 (0.9929967629482072)\n",
            "Test: [ 300/7307]\tTime  0.024 ( 0.026)\tLoss 6.0363e-01 (6.1985e-01)\tAcc@1 0.99609375 (0.9929012666112956)\n",
            "Test: [ 350/7307]\tTime  0.023 ( 0.026)\tLoss 6.1925e-01 (6.2001e-01)\tAcc@1 1.0 (0.9929776531339032)\n",
            "Test: [ 400/7307]\tTime  0.023 ( 0.025)\tLoss 6.1455e-01 (6.2011e-01)\tAcc@1 0.9921875 (0.9929765430174564)\n",
            "Test: [ 450/7307]\tTime  0.023 ( 0.025)\tLoss 6.1934e-01 (6.2035e-01)\tAcc@1 0.9921875 (0.9929583564301552)\n",
            "Test: [ 500/7307]\tTime  0.023 ( 0.025)\tLoss 6.2980e-01 (6.2045e-01)\tAcc@1 0.99609375 (0.9928580339321357)\n",
            "Test: [ 550/7307]\tTime  0.023 ( 0.025)\tLoss 6.0708e-01 (6.2037e-01)\tAcc@1 0.99609375 (0.9929531533575318)\n",
            "Test: [ 600/7307]\tTime  0.023 ( 0.025)\tLoss 6.1793e-01 (6.2047e-01)\tAcc@1 0.9921875 (0.992928452579035)\n",
            "Test: [ 650/7307]\tTime  0.023 ( 0.025)\tLoss 6.2518e-01 (6.2052e-01)\tAcc@1 0.98828125 (0.9930155529953917)\n",
            "Test: [ 700/7307]\tTime  0.024 ( 0.024)\tLoss 6.2325e-01 (6.2060e-01)\tAcc@1 0.98828125 (0.9929397735378032)\n",
            "Test: [ 750/7307]\tTime  0.024 ( 0.024)\tLoss 6.1638e-01 (6.2041e-01)\tAcc@1 0.9921875 (0.9929677097203728)\n",
            "Test: [ 800/7307]\tTime  0.026 ( 0.024)\tLoss 6.1785e-01 (6.2040e-01)\tAcc@1 0.98828125 (0.9929531445068664)\n",
            "Test: [ 850/7307]\tTime  0.025 ( 0.025)\tLoss 6.1989e-01 (6.2026e-01)\tAcc@1 0.9921875 (0.9929907829024677)\n",
            "Test: [ 900/7307]\tTime  0.025 ( 0.025)\tLoss 6.3567e-01 (6.2027e-01)\tAcc@1 0.9921875 (0.9929635474472808)\n",
            "Test: [ 950/7307]\tTime  0.025 ( 0.025)\tLoss 6.0992e-01 (6.2030e-01)\tAcc@1 0.98828125 (0.9929843585699264)\n",
            "Test: [1000/7307]\tTime  0.024 ( 0.025)\tLoss 6.3026e-01 (6.2035e-01)\tAcc@1 0.99609375 (0.9930226023976024)\n",
            "Test: [1050/7307]\tTime  0.024 ( 0.025)\tLoss 6.2137e-01 (6.2035e-01)\tAcc@1 0.9921875 (0.9930720742150333)\n",
            "Test: [1100/7307]\tTime  0.025 ( 0.024)\tLoss 6.2247e-01 (6.2031e-01)\tAcc@1 0.99609375 (0.9930248069936422)\n",
            "Test: [1150/7307]\tTime  0.024 ( 0.025)\tLoss 6.2138e-01 (6.2024e-01)\tAcc@1 0.9921875 (0.9930155842745438)\n",
            "Test: [1200/7307]\tTime  0.024 ( 0.025)\tLoss 6.1187e-01 (6.2025e-01)\tAcc@1 0.99609375 (0.99298110949209)\n",
            "Test: [1250/7307]\tTime  0.023 ( 0.024)\tLoss 6.2672e-01 (6.2024e-01)\tAcc@1 0.99609375 (0.9929774930055956)\n",
            "Test: [1300/7307]\tTime  0.023 ( 0.024)\tLoss 5.9517e-01 (6.2039e-01)\tAcc@1 0.99609375 (0.9929741544965411)\n",
            "Test: [1350/7307]\tTime  0.025 ( 0.024)\tLoss 6.3298e-01 (6.2054e-01)\tAcc@1 0.99609375 (0.992930583826795)\n",
            "Test: [1400/7307]\tTime  0.025 ( 0.024)\tLoss 6.3109e-01 (6.2054e-01)\tAcc@1 0.9921875 (0.9929263695574589)\n",
            "Test: [1450/7307]\tTime  0.024 ( 0.024)\tLoss 6.2340e-01 (6.2054e-01)\tAcc@1 1.0 (0.9928874483115093)\n",
            "Test: [1500/7307]\tTime  0.024 ( 0.024)\tLoss 6.3314e-01 (6.2051e-01)\tAcc@1 0.9921875 (0.992892758994004)\n",
            "Test: [1550/7307]\tTime  0.023 ( 0.024)\tLoss 6.2583e-01 (6.2055e-01)\tAcc@1 0.9921875 (0.9928977272727273)\n",
            "Test: [1600/7307]\tTime  0.024 ( 0.024)\tLoss 6.3026e-01 (6.2058e-01)\tAcc@1 0.9921875 (0.9929121447532792)\n",
            "Test: [1650/7307]\tTime  0.024 ( 0.024)\tLoss 6.2514e-01 (6.2061e-01)\tAcc@1 0.9921875 (0.9929114930345245)\n",
            "Test: [1700/7307]\tTime  0.024 ( 0.024)\tLoss 6.1255e-01 (6.2062e-01)\tAcc@1 0.99609375 (0.9929131760728983)\n",
            "Test: [1750/7307]\tTime  0.024 ( 0.024)\tLoss 6.2322e-01 (6.2067e-01)\tAcc@1 0.9921875 (0.9929125321245003)\n",
            "Test: [1800/7307]\tTime  0.023 ( 0.024)\tLoss 6.0654e-01 (6.2063e-01)\tAcc@1 0.9921875 (0.9929075860632982)\n",
            "Test: [1850/7307]\tTime  0.024 ( 0.024)\tLoss 5.8928e-01 (6.2060e-01)\tAcc@1 1.0 (0.9929451141274986)\n",
            "Test: [1900/7307]\tTime  0.024 ( 0.024)\tLoss 6.2110e-01 (6.2061e-01)\tAcc@1 0.98828125 (0.9929683390320884)\n",
            "Test: [1950/7307]\tTime  0.025 ( 0.024)\tLoss 6.1600e-01 (6.2059e-01)\tAcc@1 1.0 (0.9929623430292158)\n",
            "Test: [2000/7307]\tTime  0.024 ( 0.024)\tLoss 6.2816e-01 (6.2060e-01)\tAcc@1 0.98828125 (0.9929644552723638)\n",
            "Test: [2050/7307]\tTime  0.024 ( 0.024)\tLoss 6.2398e-01 (6.2050e-01)\tAcc@1 0.9921875 (0.992941705265724)\n",
            "Test: [2100/7307]\tTime  0.024 ( 0.024)\tLoss 6.1471e-01 (6.2051e-01)\tAcc@1 1.0 (0.9929535042836745)\n",
            "Test: [2150/7307]\tTime  0.024 ( 0.024)\tLoss 6.1768e-01 (6.2049e-01)\tAcc@1 0.984375 (0.992939330543933)\n",
            "Test: [2200/7307]\tTime  0.024 ( 0.024)\tLoss 6.1638e-01 (6.2051e-01)\tAcc@1 0.9921875 (0.9929346745797365)\n",
            "Test: [2250/7307]\tTime  0.024 ( 0.024)\tLoss 6.2460e-01 (6.2050e-01)\tAcc@1 0.9921875 (0.992937166814749)\n",
            "Test: [2300/7307]\tTime  0.024 ( 0.024)\tLoss 6.3475e-01 (6.2052e-01)\tAcc@1 0.99609375 (0.9929361554758801)\n",
            "Test: [2350/7307]\tTime  0.023 ( 0.024)\tLoss 6.3270e-01 (6.2051e-01)\tAcc@1 0.9921875 (0.992930202573373)\n",
            "Test: [2400/7307]\tTime  0.024 ( 0.024)\tLoss 6.1244e-01 (6.2051e-01)\tAcc@1 0.9921875 (0.9929342591628488)\n",
            "Test: [2450/7307]\tTime  0.024 ( 0.024)\tLoss 6.2733e-01 (6.2048e-01)\tAcc@1 0.99609375 (0.9929317752957977)\n",
            "Test: [2500/7307]\tTime  0.026 ( 0.024)\tLoss 6.3188e-01 (6.2052e-01)\tAcc@1 0.98828125 (0.9929247051179528)\n",
            "Test: [2550/7307]\tTime  0.024 ( 0.024)\tLoss 6.2735e-01 (6.2055e-01)\tAcc@1 0.9921875 (0.9929332247157977)\n",
            "Test: [2600/7307]\tTime  0.024 ( 0.024)\tLoss 6.0334e-01 (6.2053e-01)\tAcc@1 0.9921875 (0.9929309039792388)\n",
            "Test: [2650/7307]\tTime  0.023 ( 0.024)\tLoss 6.1341e-01 (6.2051e-01)\tAcc@1 0.9921875 (0.9929389852885704)\n",
            "Test: [2700/7307]\tTime  0.024 ( 0.024)\tLoss 6.1754e-01 (6.2049e-01)\tAcc@1 0.98046875 (0.9929395362828582)\n",
            "Test: [2750/7307]\tTime  0.024 ( 0.024)\tLoss 6.1042e-01 (6.2045e-01)\tAcc@1 0.984375 (0.9929471669392949)\n",
            "Test: [2800/7307]\tTime  0.023 ( 0.024)\tLoss 6.1895e-01 (6.2050e-01)\tAcc@1 1.0 (0.9929391846661907)\n",
            "Test: [2850/7307]\tTime  0.024 ( 0.024)\tLoss 6.3415e-01 (6.2050e-01)\tAcc@1 0.9921875 (0.9929383330410382)\n",
            "Test: [2900/7307]\tTime  0.024 ( 0.024)\tLoss 6.2638e-01 (6.2047e-01)\tAcc@1 0.99609375 (0.9929550155118925)\n",
            "Test: [2950/7307]\tTime  0.024 ( 0.024)\tLoss 6.2052e-01 (6.2044e-01)\tAcc@1 1.0 (0.9929578956286005)\n",
            "Test: [3000/7307]\tTime  0.024 ( 0.024)\tLoss 6.1153e-01 (6.2038e-01)\tAcc@1 1.0 (0.9929671880206598)\n",
            "Test: [3050/7307]\tTime  0.024 ( 0.024)\tLoss 6.1555e-01 (6.2040e-01)\tAcc@1 0.99609375 (0.9929672136184857)\n",
            "Test: [3100/7307]\tTime  0.024 ( 0.024)\tLoss 6.2621e-01 (6.2042e-01)\tAcc@1 0.98828125 (0.992949602950661)\n",
            "Test: [3150/7307]\tTime  0.024 ( 0.024)\tLoss 6.1678e-01 (6.2038e-01)\tAcc@1 0.984375 (0.9929474274039988)\n",
            "Test: [3200/7307]\tTime  0.024 ( 0.024)\tLoss 6.2545e-01 (6.2038e-01)\tAcc@1 0.99609375 (0.9929514214308028)\n",
            "Test: [3250/7307]\tTime  0.025 ( 0.024)\tLoss 6.1692e-01 (6.2038e-01)\tAcc@1 0.984375 (0.9929552926022762)\n",
            "Test: [3300/7307]\tTime  0.024 ( 0.024)\tLoss 6.2704e-01 (6.2039e-01)\tAcc@1 0.9921875 (0.992937746137534)\n",
            "Test: [3350/7307]\tTime  0.024 ( 0.024)\tLoss 6.2340e-01 (6.2039e-01)\tAcc@1 0.99609375 (0.9929347116532379)\n",
            "Test: [3400/7307]\tTime  0.024 ( 0.024)\tLoss 6.1692e-01 (6.2037e-01)\tAcc@1 0.99609375 (0.9929421034254631)\n",
            "Test: [3450/7307]\tTime  0.024 ( 0.024)\tLoss 6.2302e-01 (6.2036e-01)\tAcc@1 0.99609375 (0.9929424894957983)\n",
            "Test: [3500/7307]\tTime  0.024 ( 0.024)\tLoss 6.3824e-01 (6.2036e-01)\tAcc@1 1.0 (0.9929372857754927)\n",
            "Test: [3550/7307]\tTime  0.023 ( 0.024)\tLoss 6.1794e-01 (6.2036e-01)\tAcc@1 0.98828125 (0.9929344286820614)\n",
            "Test: [3600/7307]\tTime  0.028 ( 0.024)\tLoss 6.2670e-01 (6.2037e-01)\tAcc@1 0.9921875 (0.9929435833796167)\n",
            "Test: [3650/7307]\tTime  0.025 ( 0.024)\tLoss 6.1094e-01 (6.2039e-01)\tAcc@1 0.99609375 (0.9929310890851821)\n",
            "Test: [3700/7307]\tTime  0.024 ( 0.024)\tLoss 6.1841e-01 (6.2041e-01)\tAcc@1 0.99609375 (0.9929337087949203)\n",
            "Test: [3750/7307]\tTime  0.024 ( 0.024)\tLoss 6.0549e-01 (6.2039e-01)\tAcc@1 0.99609375 (0.9929414656091708)\n",
            "Test: [3800/7307]\tTime  0.023 ( 0.024)\tLoss 6.3270e-01 (6.2041e-01)\tAcc@1 0.9921875 (0.992956212181005)\n",
            "Test: [3850/7307]\tTime  0.025 ( 0.024)\tLoss 6.2676e-01 (6.2039e-01)\tAcc@1 0.9921875 (0.9929685471306154)\n",
            "Test: [3900/7307]\tTime  0.025 ( 0.024)\tLoss 6.2785e-01 (6.2041e-01)\tAcc@1 0.98828125 (0.9929545308895155)\n",
            "Test: [3950/7307]\tTime  0.024 ( 0.024)\tLoss 6.0962e-01 (6.2043e-01)\tAcc@1 1.0 (0.9929744843077701)\n",
            "Test: [4000/7307]\tTime  0.024 ( 0.024)\tLoss 6.3284e-01 (6.2044e-01)\tAcc@1 0.9921875 (0.992976365283679)\n",
            "Test: [4050/7307]\tTime  0.024 ( 0.024)\tLoss 6.3407e-01 (6.2044e-01)\tAcc@1 1.0 (0.9929849497037768)\n",
            "Test: [4100/7307]\tTime  0.025 ( 0.024)\tLoss 6.0944e-01 (6.2042e-01)\tAcc@1 0.99609375 (0.9929914197756645)\n",
            "Test: [4150/7307]\tTime  0.024 ( 0.024)\tLoss 6.1548e-01 (6.2042e-01)\tAcc@1 0.99609375 (0.9929864415201156)\n",
            "Test: [4200/7307]\tTime  0.024 ( 0.024)\tLoss 6.0951e-01 (6.2042e-01)\tAcc@1 0.98828125 (0.992977862413711)\n",
            "Test: [4250/7307]\tTime  0.024 ( 0.024)\tLoss 6.3441e-01 (6.2045e-01)\tAcc@1 0.99609375 (0.992967647318278)\n",
            "Test: [4300/7307]\tTime  0.023 ( 0.024)\tLoss 6.3440e-01 (6.2044e-01)\tAcc@1 0.9921875 (0.9929849162985352)\n",
            "Test: [4350/7307]\tTime  0.024 ( 0.024)\tLoss 6.1638e-01 (6.2044e-01)\tAcc@1 0.9921875 (0.9929865260859573)\n",
            "Test: [4400/7307]\tTime  0.025 ( 0.024)\tLoss 6.1842e-01 (6.2044e-01)\tAcc@1 0.99609375 (0.9929889868779823)\n",
            "Test: [4450/7307]\tTime  0.025 ( 0.024)\tLoss 6.2669e-01 (6.2045e-01)\tAcc@1 0.9921875 (0.9929966580543698)\n",
            "Test: [4500/7307]\tTime  0.024 ( 0.024)\tLoss 6.1254e-01 (6.2045e-01)\tAcc@1 0.99609375 (0.9930058945234392)\n",
            "Test: [4550/7307]\tTime  0.024 ( 0.024)\tLoss 6.1778e-01 (6.2046e-01)\tAcc@1 1.0 (0.9930114947264338)\n",
            "Test: [4600/7307]\tTime  0.025 ( 0.024)\tLoss 6.3899e-01 (6.2045e-01)\tAcc@1 0.9921875 (0.9930229162138665)\n",
            "Test: [4650/7307]\tTime  0.023 ( 0.024)\tLoss 5.9510e-01 (6.2044e-01)\tAcc@1 0.99609375 (0.9930223339066867)\n",
            "Test: [4700/7307]\tTime  0.024 ( 0.024)\tLoss 6.0484e-01 (6.2045e-01)\tAcc@1 0.984375 (0.9930068070623271)\n",
            "Test: [4750/7307]\tTime  0.024 ( 0.024)\tLoss 6.1241e-01 (6.2045e-01)\tAcc@1 0.98828125 (0.9930022955693538)\n",
            "Test: [4800/7307]\tTime  0.025 ( 0.024)\tLoss 6.3325e-01 (6.2043e-01)\tAcc@1 0.9921875 (0.9930157779629244)\n",
            "Test: [4850/7307]\tTime  0.024 ( 0.024)\tLoss 6.2346e-01 (6.2042e-01)\tAcc@1 1.0 (0.9930225404555761)\n",
            "Test: [4900/7307]\tTime  0.025 ( 0.024)\tLoss 6.2997e-01 (6.2043e-01)\tAcc@1 0.98828125 (0.9930108332483166)\n",
            "Test: [4950/7307]\tTime  0.024 ( 0.024)\tLoss 6.1520e-01 (6.2043e-01)\tAcc@1 0.99609375 (0.9929993625025247)\n",
            "Test: [5000/7307]\tTime  0.024 ( 0.024)\tLoss 6.2379e-01 (6.2048e-01)\tAcc@1 0.98828125 (0.9930021808138373)\n",
            "Test: [5050/7307]\tTime  0.024 ( 0.024)\tLoss 6.2329e-01 (6.2048e-01)\tAcc@1 0.9921875 (0.9930119035834488)\n",
            "Test: [5100/7307]\tTime  0.024 ( 0.024)\tLoss 6.0950e-01 (6.2049e-01)\tAcc@1 0.99609375 (0.9930061201235052)\n",
            "Test: [5150/7307]\tTime  0.024 ( 0.024)\tLoss 6.1257e-01 (6.2050e-01)\tAcc@1 0.9921875 (0.9930065157251019)\n",
            "Test: [5200/7307]\tTime  0.024 ( 0.024)\tLoss 6.2614e-01 (6.2049e-01)\tAcc@1 0.99609375 (0.9930129121803499)\n",
            "Test: [5250/7307]\tTime  0.024 ( 0.024)\tLoss 6.3681e-01 (6.2049e-01)\tAcc@1 1.0 (0.9930199307274805)\n",
            "Test: [5300/7307]\tTime  0.024 ( 0.024)\tLoss 6.2382e-01 (6.2050e-01)\tAcc@1 0.9921875 (0.9930268168741747)\n",
            "Test: [5350/7307]\tTime  0.024 ( 0.024)\tLoss 6.2340e-01 (6.2049e-01)\tAcc@1 1.0 (0.9930218942720986)\n",
            "Test: [5400/7307]\tTime  0.024 ( 0.024)\tLoss 6.0006e-01 (6.2049e-01)\tAcc@1 0.98828125 (0.9930120000925754)\n",
            "Test: [5450/7307]\tTime  0.024 ( 0.024)\tLoss 6.1099e-01 (6.2050e-01)\tAcc@1 1.0 (0.9930173362685746)\n",
            "Test: [5500/7307]\tTime  0.024 ( 0.024)\tLoss 6.2196e-01 (6.2050e-01)\tAcc@1 1.0 (0.9930318067169606)\n",
            "Test: [5550/7307]\tTime  0.024 ( 0.024)\tLoss 6.2231e-01 (6.2053e-01)\tAcc@1 0.99609375 (0.9930185721041254)\n",
            "Test: [5600/7307]\tTime  0.024 ( 0.024)\tLoss 6.2586e-01 (6.2053e-01)\tAcc@1 0.9765625 (0.9930097583020889)\n",
            "Test: [5650/7307]\tTime  0.025 ( 0.024)\tLoss 6.2818e-01 (6.2055e-01)\tAcc@1 0.9921875 (0.9930011004689435)\n",
            "Test: [5700/7307]\tTime  0.024 ( 0.024)\tLoss 6.2535e-01 (6.2056e-01)\tAcc@1 0.9921875 (0.9929980759954394)\n",
            "Test: [5750/7307]\tTime  0.024 ( 0.024)\tLoss 6.2447e-01 (6.2056e-01)\tAcc@1 0.9921875 (0.9929971418014258)\n",
            "Test: [5800/7307]\tTime  0.024 ( 0.024)\tLoss 6.1745e-01 (6.2057e-01)\tAcc@1 0.9921875 (0.9929935302103086)\n",
            "Test: [5850/7307]\tTime  0.024 ( 0.024)\tLoss 6.2489e-01 (6.2058e-01)\tAcc@1 0.99609375 (0.9929913155870791)\n",
            "Test: [5900/7307]\tTime  0.025 ( 0.024)\tLoss 6.2194e-01 (6.2055e-01)\tAcc@1 1.0 (0.9929911243856974)\n",
            "Test: [5950/7307]\tTime  0.024 ( 0.024)\tLoss 6.3783e-01 (6.2055e-01)\tAcc@1 0.984375 (0.9929856851789615)\n",
            "Test: [6000/7307]\tTime  0.024 ( 0.024)\tLoss 6.1977e-01 (6.2055e-01)\tAcc@1 0.9921875 (0.9929770819446759)\n",
            "Test: [6050/7307]\tTime  0.025 ( 0.024)\tLoss 6.2430e-01 (6.2055e-01)\tAcc@1 0.9921875 (0.9929686208891092)\n",
            "Test: [6100/7307]\tTime  0.024 ( 0.024)\tLoss 6.2582e-01 (6.2055e-01)\tAcc@1 0.9921875 (0.9929686219472218)\n",
            "Test: [6150/7307]\tTime  0.023 ( 0.024)\tLoss 6.3623e-01 (6.2056e-01)\tAcc@1 0.99609375 (0.9929552867419932)\n",
            "Test: [6200/7307]\tTime  0.025 ( 0.024)\tLoss 6.3038e-01 (6.2055e-01)\tAcc@1 0.99609375 (0.9929661042573779)\n",
            "Test: [6250/7307]\tTime  0.024 ( 0.024)\tLoss 6.2768e-01 (6.2055e-01)\tAcc@1 0.99609375 (0.9929786234202528)\n",
            "Test: [6300/7307]\tTime  0.024 ( 0.024)\tLoss 6.2297e-01 (6.2056e-01)\tAcc@1 0.98828125 (0.9929822647198857)\n",
            "Test: [6350/7307]\tTime  0.025 ( 0.024)\tLoss 6.1750e-01 (6.2055e-01)\tAcc@1 1.0 (0.9929815432609038)\n",
            "Test: [6400/7307]\tTime  0.024 ( 0.024)\tLoss 6.2231e-01 (6.2053e-01)\tAcc@1 0.9921875 (0.9929857151226371)\n",
            "Test: [6450/7307]\tTime  0.024 ( 0.024)\tLoss 6.2510e-01 (6.2052e-01)\tAcc@1 0.9921875 (0.9929910333669199)\n",
            "Test: [6500/7307]\tTime  0.023 ( 0.024)\tLoss 6.1599e-01 (6.2051e-01)\tAcc@1 0.99609375 (0.9929848532918013)\n",
            "Test: [6550/7307]\tTime  0.024 ( 0.024)\tLoss 6.2227e-01 (6.2051e-01)\tAcc@1 0.9921875 (0.9929930783468173)\n",
            "Test: [6600/7307]\tTime  0.024 ( 0.024)\tLoss 6.1923e-01 (6.2053e-01)\tAcc@1 0.9921875 (0.9929946693682775)\n",
            "Test: [6650/7307]\tTime  0.023 ( 0.024)\tLoss 6.2436e-01 (6.2054e-01)\tAcc@1 0.984375 (0.9929950618328071)\n",
            "Test: [6700/7307]\tTime  0.025 ( 0.024)\tLoss 6.2882e-01 (6.2054e-01)\tAcc@1 0.99609375 (0.9929907849574691)\n",
            "Test: [6750/7307]\tTime  0.024 ( 0.024)\tLoss 6.0708e-01 (6.2056e-01)\tAcc@1 0.99609375 (0.992994672085617)\n",
            "Test: [6800/7307]\tTime  0.024 ( 0.024)\tLoss 6.2205e-01 (6.2057e-01)\tAcc@1 0.99609375 (0.9929927584178797)\n",
            "Test: [6850/7307]\tTime  0.024 ( 0.024)\tLoss 6.1192e-01 (6.2057e-01)\tAcc@1 0.98828125 (0.9929885919938695)\n",
            "Test: [6900/7307]\tTime  0.024 ( 0.024)\tLoss 6.1153e-01 (6.2058e-01)\tAcc@1 0.99609375 (0.9929873161498334)\n",
            "Test: [6950/7307]\tTime  0.024 ( 0.024)\tLoss 6.1099e-01 (6.2057e-01)\tAcc@1 0.99609375 (0.992984372752122)\n",
            "Test: [7000/7307]\tTime  0.024 ( 0.024)\tLoss 6.2016e-01 (6.2055e-01)\tAcc@1 0.99609375 (0.9929842611769747)\n",
            "Test: [7050/7307]\tTime  0.023 ( 0.024)\tLoss 6.2826e-01 (6.2056e-01)\tAcc@1 0.9921875 (0.9929802731882003)\n",
            "Test: [7100/7307]\tTime  0.024 ( 0.024)\tLoss 6.0651e-01 (6.2055e-01)\tAcc@1 0.98828125 (0.9929867932333474)\n",
            "Test: [7150/7307]\tTime  0.024 ( 0.024)\tLoss 6.2102e-01 (6.2056e-01)\tAcc@1 0.984375 (0.9929948608586212)\n",
            "Test: [7200/7307]\tTime  0.024 ( 0.024)\tLoss 6.1441e-01 (6.2056e-01)\tAcc@1 0.99609375 (0.9929908823427301)\n",
            "Test: [7250/7307]\tTime  0.024 ( 0.024)\tLoss 6.2826e-01 (6.2055e-01)\tAcc@1 0.9921875 (0.9929896522893394)\n",
            "Test: [7300/7307]\tTime  0.023 ( 0.024)\tLoss 6.1816e-01 (6.2054e-01)\tAcc@1 0.9921875 (0.9930039549376798)\n",
            "Test: [7306/7307]\tTime  0.087 ( 0.024)\tLoss 6.1369e-01 (6.2054e-01)\tAcc@1 0.9869281045751634 (0.9930055723396395)\n",
            "/content/LilNetX/trainer.py:1008: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  'Net Bytes {bytes}'.format(bytes=bits//8000) if not conf_network['vanilla'] else ''+ \\\n",
            " * Acc@1 0.9930055723396395Net Bytes 12.0\n",
            "/content/LilNetX/trainer.py:1011: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  self.wandb.log({\"loss_val\": losses.avg, \"top1_val\": top1.avg, \"net_bytes_val\": bits//8000, \\\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      ac_bytes ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        f1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: net_bytes_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      prec_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    recall_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   roc_auc_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      top1_val ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              ac_bytes 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_bytes 57688\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            best_epoch 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         best_val_top1 0.94478\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 epoch 14\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              f1_train 0.57959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                f1_val 0.98258\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           final_bytes 18\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        final_val_top1 0.81298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_reg_train 3e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            loss_train 0.65996\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              loss_val 0.62054\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    lr 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             net_bytes 46.98259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         net_bytes_val 12.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            prec_train 0.81628\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              prec_val 0.97659\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          recall_train 0.45184\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            recall_val 0.98899\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         roc_auc_train 0.70392\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           roc_auc_val 0.99573\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        sparse_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       sparse_discrete 0.92425\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     sparse_in_decoded 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_in_discrete 0.0303\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    sparse_out_decoded 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   sparse_out_discrete 0.36458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  sparse_slice_decoded 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: sparse_slice_discrete 0.73413\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               timings 183.69458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            top1_train 0.80099\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              top1_val 0.99301\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcnn_ids_lilnetx_fold0\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/lfml/cnn_ids_lilnetx/runs/c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 3 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold4/wandb/run-20221216_042200-c0ee252573fd6829c5a76dcdd33c55df/logs\u001b[0m\n",
            "Total time taken: 220.75s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference time"
      ],
      "metadata": {
        "id": "j68lKB4bVKjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()\n",
        "!python3 LilNetX/main.py --config LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_0.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlEn7AEWVLXF",
        "outputId": "d723b897-ca5b-4bf5-c1dd-5e5874383abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config loaded from:  /content/LilNetX/configs/test_avtp/avtp_intrusion_cnn_ids_test_0.yaml\n",
            "Running evaluation only\n",
            "Changed random seed to 964036\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id c0ee252573fd6829c5a76dcdd33c55df.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to \u001b[1m`offline`\u001b[0m in this directory.  \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb online`\u001b[0m or set \u001b[1mWANDB_MODE=online\u001b[0m to enable cloud syncing.\n",
            "tcmalloc: large alloc 3283222528 bytes == 0x706e000 @  0x7f92cd2eb1e7 0x7f926ae0814e 0x7f926ae60745 0x7f926ae0b9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4bddd1 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Trainset has 446372 bening Xis and 196892 injected Xis\n",
            "tcmalloc: large alloc 9546981376 bytes == 0xcc9f4000 @  0x7f92cd2eb1e7 0x7f926ae0814e 0x7f926ae60745 0x7f926ae0b9c8 0x5d8bb5 0x55ea20 0x55cd91 0x5d8941 0x49abe4 0x5d8868 0x4c8291 0x518079 0x498014 0x518079 0x55e284 0x5d8868 0x597136 0x4b8c8c 0x55e029 0x5d8868 0x4990ca 0x5d8868 0x4997a2 0x5d8868 0x5da092 0x586de6 0x5d8cdf 0x55dc1e 0x5d8868 0x4990ca 0x55cd91\n",
            "Testset has 1494253 bening Xis and 376236 injected Xis\n",
            "convnetids var = 0.003906249999999999\n",
            "classname = Conv2d, fan = 800, boundary = 0.9730919862656238\n",
            "classname = Conv2d, fan = 1600, boundary = 0.6000000000000001\n",
            "classname = Linear, fan = 64, boundary = 4.424428900898053\n",
            "classname = Linear, fan = 1, boundary = 38.69502519453203\n",
            "Number of parameters: 1.3605 M\n",
            "Checkpoint found, continuing training from epoch 10\n",
            "Changing random seed to 964036\n",
            "Mean inference time = 2273.560084402561\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[1mwandb sync /content/drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold0/wandb/offline-run-20221218_050847-c0ee252573fd6829c5a76dcdd33c55df\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./drive/MyDrive/data/checkpoints/cnn_ids_lilnetx_fold0/wandb/offline-run-20221218_050847-c0ee252573fd6829c5a76dcdd33c55df/logs\u001b[0m\n",
            "Total time taken: 130.42s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove repository folder"
      ],
      "metadata": {
        "id": "-MjCQVk-ZHTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf LilNetX"
      ],
      "metadata": {
        "id": "kgvM-WJDbn4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf results/cifar10_r20"
      ],
      "metadata": {
        "id": "P60aM7HyZGbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JNucGHZkuSjL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}